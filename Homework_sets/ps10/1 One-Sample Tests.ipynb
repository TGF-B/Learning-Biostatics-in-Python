{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "5a23859729f6ffd9ff45aa4560b1c840",
                    "grade": false,
                    "grade_id": "cell-19ad2d8f9a699924",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "# Question 1: One-Sample Tests\n",
                "\n",
                "Using the logic of \"resampling under the null hypothesis\", we will use our resampling tools from the previous question to compute p-values for some simple tests.\n",
                "\n",
                "## Problem 1.0\n",
                "Below, paste in your answers for the `resample_means` and `fraction_extreme` functions from the last question. \n",
                "\n",
                "Also, recall that the process of resampling data under the null hypothesis means *forcing* your dataset to comply with the null hypothesis, usually by shifting the data around to have zero mean (or a zero value for some other statistic). To help with this, write a function called `shift` that takes a list of values and a `shift_factor` and returns a new list where `shift_factor` has been subtracted from each of the original values. (Style points for using the \"list comprehension\" syntax to create the new list from the old one, e.g. `[x**2 for x in y]`.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "e835492ded8894d372e0d0ceefbb5196",
                    "grade": false,
                    "grade_id": "cell-d4e07ef73e0f38de",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "def mean(v):\n",
                "    return sum(v)/len(v)\n",
                "\n",
                "def resample_means(data, n_resamples):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "def fraction_extreme(values, below=None, above=None):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "def shift(values, shift_factor):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "data = [1, 2, 3, 4]\n",
                "m = mean(data)\n",
                "shifted = shift(data, m)\n",
                "print(m, mean(shifted))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "5446b8e2dc6366ec4bf9c97e00afb909",
                    "grade": true,
                    "grade_id": "cell-13213bc408702487",
                    "locked": true,
                    "points": 1,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert mean(shifted) == 0\n",
                "assert shifted == [-1.5, -0.5, 0.5, 1.5]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "ad8ab1fbe22a2377c8dc9bdede4e0358",
                    "grade": false,
                    "grade_id": "cell-3e9427c5e1dcac39",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Let's first examine the experiment discussed in class to determine whether rats have a left-hand bias. The experimental setup is as follows. We put each of a number of rats in a cage, and let it press on a lever 20 times. Each time it presses on the lever with its left paw, we add -1 to a running total, and each time it presses with its right paw, we add +1 to the total. So a rat with a 100% left-hand bias will have a score of -20, and a rat with no bias will have a score that averages around 0.\n",
                "\n",
                "The below `rat_biases` list has scores for 24 rats. Its mean is -2, indicating that the average rat pressed the bar two more times with its left paw than its right (i.e. 11 left-paw presses and 9 right-paw presses.) Is this indicative of something more than the expected amount of random variability in a small sample?\n",
                "\n",
                "## Problem 1.1\n",
                "We will first answer this question by **simulation**. Write a function, `simulate_rat_trial`, that will simulate 20 presses on a bar for each of 24 rats, assuming no left/right bias, and return the mean score. (To simulate 20 presses, it might be simplest to use `random.choices` to draw from `[-1, 1]` 20 times, and then use `sum` to calculate the total bias score from those 20 presses.)\n",
                "\n",
                "Next write a function, `simulate_rat_trials`, that will run `simulate_rat_trial` a specified number of times and return a list of the mean bias scores for each trial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "335fd198fd3c766a331a1749b23706e5",
                    "grade": false,
                    "grade_id": "cell-01fae9f56c61c29d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "rat_biases = [-2,  -2,   8,   6,  -2, -12,  -4,  -2,  -6,   0,  -4,  -8,\n",
                "              -4,   2,   2,  -2,   2,  -4,  -4,  -4,   2,  -2,  -8,   0]\n",
                "print('mean bias', mean(rat_biases))\n",
                "\n",
                "def simulate_rat_trial():\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "print(\"trial 1:\", simulate_rat_trial())\n",
                "print(\"trial 2:\", simulate_rat_trial())\n",
                "\n",
                "def simulate_rat_trials(n_trials):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "simulated_trials = simulate_rat_trials(5000)\n",
                "print('range of simulated trials:', min(simulated_trials), max(simulated_trials))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4e91a74ef2a1fb5d688b7422eab6149d",
                    "grade": true,
                    "grade_id": "cell-cb10e00650815646",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert len(simulated_trials) == 5000\n",
                "assert -0.05 < mean(simulated_trials) < 0.05\n",
                "for s in simulated_trials:\n",
                "    assert (s * 24) == int(s * 24) # each mean should be a sum of integers divided by 24...\n",
                "assert min(simulated_trials) < -2.5\n",
                "assert max(simulated_trials) > 2.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "0e80e1d049136fc6c3231521acff3275",
                    "grade": false,
                    "grade_id": "cell-6c030a81050d0091",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So, the simulation shows that even in a no-bias condition, it's possible for random samples of 24 rats to have a wide range of possible mean bias scores! In 5000 repeat trials, you should have gotten some with bias scores as low as -3 and as high as 3 (or so). So we know that even if rats use each paw equally on average, it is still *possible* to get a score of -2, such as we observed in the actual rat data. But how *likely* is that? Recall that \"the probability of observing a result as or more extreme by chance alone\" is the definition of the p-value, so let's calculate a p-value for the observed bias score based on these simulation results.\n",
                "\n",
                "## Problem 1.2\n",
                "Use the function `fraction_extreme` to calculate the p-value of obtaining a bias score as far or farther from zero than the one we actually observed. Store this in a variable `p_value`.\n",
                "\n",
                "Next, plot the null distribution: the histogram of scores from your simulated trials where there was no bias. Last, plot a vertical line at the position of the actual observed bias score.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "0bc397771d4384719311ec877f346ca6",
                    "grade": false,
                    "grade_id": "cell-348e76f99616ba60",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "from matplotlib import pyplot as plt\n",
                "plt.style.use('ggplot')\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "print('p =', p_value)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "07e1bf28bb0c7f19023ea8782fd80bc2",
                    "grade": true,
                    "grade_id": "cell-062dc93cb4fe333a",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.01 < p_value < 0.05"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "f3414d5556e42abfd0993731f1a5c4fa",
                    "grade": false,
                    "grade_id": "cell-de681d22ae5a6e85",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So, our simulation shows that while it's *possible* to get a bias score more extreme than -2, it happens less than 5% of the time. (If you run the simulation repeatedly, you should find that scores that extreme or more occur generally around 3% or so of the time.) So we would say that the result is \"significantly unlikely to be due to chance alone\", or in shorthand, \"statistically significant\". (Using the p=0.05 threshold favored by biologists.)\n",
                "\n",
                "Now, what about bootstrapping? How does that compare to running a simulation? Under the simulation, we made new data that complied with the null hypothesis (of no left-right bias) by construction. On the other hand, our existing dataset has a slight (but apparently significant) bias. How can we resample these data  to generate a null distribution?\n",
                "\n",
                "As discussed in class, we simply shift the location of the data points so that the null hypothesis is true, and then repeatedly resample to generate a null distribution.  \n",
                "\n",
                "## Problem 1.3\n",
                "Write a function `bootstrap_mean`, that takes a data set and shifts it to have zero mean (using your `shift` function), and then uses your `resample_means` function to resample the data a given number of times and return the mean of each resampled dataset. Last, calculate a p-value as `bootstrap_p_value`, and plot the null distribution and a vertical line at the actual bias score of -2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "1057d1888a4bf1ebe27100eb65672618",
                    "grade": false,
                    "grade_id": "cell-8385e2fc0dc06fcd",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def bootstrap_mean(data, n_resamples):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "bootstrapped_trials = bootstrap_mean(rat_biases, 5000)\n",
                "print('range of bootstrapped trials:', min(bootstrapped_trials), max(bootstrapped_trials))\n",
                "\n",
                "# Now calculate p-value and plot histogram of the null distribution\n",
                "# YOUR ANSWER HERE\n",
                "print('p =', bootstrap_p_value)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "d472422d51454ab689ea5bce7d2e861b",
                    "grade": true,
                    "grade_id": "cell-bfc96f6c7dafc18f",
                    "locked": true,
                    "points": 4,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert bootstrap_mean([1,1,1,1], 200) == [0]*200\n",
                "assert len(bootstrapped_trials) == 5000\n",
                "assert -0.05 < mean(bootstrapped_trials) < 0.05\n",
                "for b in bootstrapped_trials:\n",
                "    assert (b * 24) == int(b * 24) # each mean should be a sum of integers divided by 24...\n",
                "assert min(bootstrapped_trials) < -2.5\n",
                "assert max(bootstrapped_trials) > 2.5\n",
                "assert 0.01 < bootstrap_p_value < 0.05"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "6a103e10c7bc91bb98370e154be1ca3d",
                    "grade": false,
                    "grade_id": "cell-8b45c99a9f0895e1",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Wow! The bootstrap procedure produces results that are extremely similar to the exact simulation. (Note that the samller your original dataset is, the worse the bootstrap will perform compared to simulating new data or compared to using parametric statistics.) Even better, though, is that the bootstrap can be used in situations where it's not obvious how to write a simulation.\n",
                "\n",
                "For an example of this, consider the question posed in class about whether attending a tutorial session is guaranteed to boost a student's homework score. (Imagine that we graded the homework first before and then after the tutorial session, so we could measure score increase.)\n",
                "\n",
                "Let's assume that of 30 students who attended the tutorial session, each of them saw an improvement in score of at least one point. That is, the minimum *observed* improvement was > 0.\n",
                "\n",
                "But how likely would it be to get a sample of 30 students who all had improvements > 0 in the case that there was a theoretical possibility of having no improvement? (I.e. under the null hypothesis that the minimum possible  improvement is zero.)\n",
                "\n",
                "How would you simulate this situation? You would have to have some idea of how the learning process works in order to  model the distribution of score improvements under the null hypothesis! This would not be particularly simple to do...\n",
                "\n",
                "Bootstrapping allows us to short-circuit this whole issue. After all, we have a perfectly good example of the distribution of improvements: those that we observed. The only problem is that the minimum improvement is 1 point, while the null hypothesis requires that the minimum be 0. So to generate new data under the null hypothesis, all we have to do is shift the observed distribution to have a minimum of 0 and resample from that.\n",
                "\n",
                "## Problem 1.4\n",
                "A list of 30 score improvements is provided below. Note that most students improved a lot, but two stragglers had an improvement of only one point. Write a function `bootstrap_minimum` that takes a list of improvements, shifts it so that its *minimum* is zero, and then performs a specified number of resamplings. For each resampled dataset, calculate its *minimum*, and return a list of all the calculated minima.\n",
                "\n",
                "Then use the resampled data to calculate the probability of observing a minimum improvement of 1 or larger across 30 students, under the null hypothesis that the theoretical minimum improvement is zero. (I.e. calculate the p-value.) Store this as `improvement_p_value`. Note that this will be a one-tailed p-value: the fraction of scores that go up by one point or more. (As the tutorial session never seems to make homework scores worse, we needn't bother examining the case that scores go down by one point or more...)\n",
                "\n",
                "Finally, it would be good to visualize the null distribution of improvement scores. Unfortunately, in this case the distribution is quite skewed (you'll see). This makes it hard to see the true shape of the distribution from a histogram, which would be dominated by one huge bin.\n",
                "\n",
                "The good news is that we know that each minimum must be an integer >=0, so we can just count how many 0s we see, how many 1s, and so forth, up to the maximum value seen in the list of `bootstrapped_minima`. To do this, write a function called `bincount` that will count the number of occurences of each integer in a list. This function should create a list of counters: one for each integer from zero to the maximum value in the input data. The function should then step through the input values in a loop and, for each value, increment the corresponding counter.\n",
                "\n",
                "For example, consider the input `[1,2,3,1]`. In this list, there are no 0s, two 1s, one 2 and one 3. Thus, the bincount should be `[0, 2, 1, 1]`. If the input is `[0,2,0,0]`, then the bincount should be `[3, 0, 1]` (i.e. three 0s, no 1s, and one 2).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "20adc546497575d5fe1f37acbf1bd8d1",
                    "grade": false,
                    "grade_id": "cell-01d6ddbbb1976d82",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "improvements = [17,  8,  5,  1,  9, 11,  8,  1, 11,  8,  6,  6,  9, 11, 10,\n",
                "                12,  5, 18, 19,  4,  5,  9, 15, 11,  9, 17, 16, 10, 16, 11]\n",
                "\n",
                "print('mean improvement', mean(improvements))\n",
                "\n",
                "def bootstrap_minimum(data, n_resamples):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "bootstrapped_minima = bootstrap_minimum(improvements, 10000)\n",
                "print('range of bootstrapped minima:', min(bootstrapped_minima), max(bootstrapped_minima))\n",
                "\n",
                "# Now calculate p-value \n",
                "# YOUR ANSWER HERE\n",
                "print('p =', improvement_p_value)\n",
                "print() # print a blank line\n",
                "\n",
                "def bincount(data):\n",
                "    num_bins = max(data) + 1\n",
                "    # If the maximum value is 3, we need to keep a count of the number of 0s, 1s, 2s, and 3s.\n",
                "    # Thus, we need 4 total bins. Every time we see a 0 in the data, we should increment the\n",
                "    # counter in the 0th position of the counts list. Every time we see a 1, we should increment\n",
                "    # the counter in the 1st position, and so forth...\n",
                "    counters = [0] * num_bins\n",
                "    # YOUR ANSWER HERE\n",
                "    return counters\n",
                "\n",
                "print('bincount tests:')\n",
                "print(bincount([1,1,2,3])) # should be [0, 2, 1, 1]\n",
                "print(bincount([0,0,0,2])) # should be [3, 0, 1]\n",
                "print() # print a blank line\n",
                "\n",
                "print('bootstrapped minima counts:')\n",
                "minima_counts = bincount(bootstrapped_minima)\n",
                "print(minima_counts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "e21530c5cf3fa0154d526c181ffe7c49",
                    "grade": true,
                    "grade_id": "cell-44b0e50d03faebe4",
                    "locked": true,
                    "points": 5,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert bootstrap_minimum([1,1,1,1], 200) == [0]*200\n",
                "assert len(bootstrapped_minima) == 10000\n",
                "assert min(bootstrapped_minima) == 0\n",
                "shifted_improvements = set(shift(improvements, 1))\n",
                "for b in bootstrapped_minima:\n",
                "    assert b in shifted_improvements\n",
                "assert max(bootstrapped_minima) < 10\n",
                "assert 0.12 < improvement_p_value < 0.14\n",
                "counts = bincount([0]*50 + [999])\n",
                "assert len(counts) == 1000\n",
                "assert counts[0] == 50\n",
                "assert counts[999] == 1\n",
                "assert sum(counts) == 51\n",
                "assert abs(improvement_p_value - sum(minima_counts[1:])/10000) < 0.0000001"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "d2c53b0db9385afccd53403721e44461",
                    "grade": false,
                    "grade_id": "cell-1642b2d3f276c742",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "To recap: we have resampled data under the scenario that score improvements are distributed basically as we observed, except that the minimum value could be zero. In this case, it is still reasonably common for a set of 30 students to all improve by at least one point, just by chance alone. (In particular, this happens about 13% of the time.)\n",
                "\n",
                "So that means that observing data such as we did, where two students improved by one point and the rest by more, is not really a strong indication that the minimum theoretical improvement is greater than zero.\n",
                "\n",
                "In other words, given these data, we cannot say that the null hypothesis (that the tutoring sessions don't raise everyone's scores) is particularly unlikely. \n",
                "\n",
                "## Problem 1.5\n",
                "What if we observed slightly different data? In particular, what if **more** students saw a one-point improvement and fewer saw larger improvements? The minimum improvement observed would still be one, and the mean would be decreased somewhat. It seems like in this scenario it could only be *less likely* that the tutorial session was helping everyone improve, compared to the previous scenario where more students improved by more than 1 point, right?\n",
                "\n",
                "Test this intuition out on the `improvements2` datset, where 5 students saw an improvement of 1 point. Calculate a p-value as `improvement2_p_value`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "6e737bf90efdd777ed264f2bc35d260a",
                    "grade": false,
                    "grade_id": "cell-33caaf50ea2cbd47",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "improvements2 = [1,  8,  5,  1,  9, 11,  8,  1, 11,  8,  6,  6,  9, 11, 10,\n",
                "                12,  5, 18, 19,  4,  5,  9, 1, 11,  9, 1, 16, 10, 16, 11]\n",
                "\n",
                "\n",
                "print('mean improvement 2', mean(improvements2))\n",
                "\n",
                "bootstrapped_minima2 = bootstrap_minimum(improvements2, 10000)\n",
                "print('range of bootstrapped minima 2:', min(bootstrapped_minima2), max(bootstrapped_minima2))\n",
                "\n",
                "# Now calculate p-value\n",
                "# YOUR ANSWER HERE\n",
                "print('p =', improvement2_p_value)\n",
                "minima2_counts = bincount(bootstrapped_minima2)\n",
                "print(minima2_counts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "65a5ff2ee17e17ce294a4c6fdfcb809f",
                    "grade": true,
                    "grade_id": "cell-a724b2c7b186c135",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.002 < improvement2_p_value < 0.006\n",
                "assert abs(improvement2_p_value - sum(minima2_counts[1:])/10000) < 0.0000001"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "36f1955eab9006acf1206d808ef2fa01",
                    "grade": false,
                    "grade_id": "cell-2e539c2d8f91c937",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "***What what what?***\n",
                "\n",
                "How could it be that with a set of observed scores where *more* students had a *smaller* improvement, we are nevertheless more certain that tutoring has a significant effect on the minimum level of improvement?\n",
                "\n",
                "The intuitive answer is that if you see a lot of students with a 1-point improvement but none with a 0-point improvement, you might imagine that 1 point is a hard floor. (Imagine observing 1000 students with a 1-point improvement and none with a zero point improvement. It would be hard to claim that these data support the hypothesis that that it's possible to not improve your score at all.) On the other hand, if you see just a few instances of a 1-point improvement, it's much harder to be certain whether that is the real lower bound, or if an even-lower bound might be possible.\n",
                "\n",
                "More formally, what the bootstrapping shows is that when there are a lot of values at the true minimum, then most of the time a new sample *will contain* that true minimum value. So when we shifted the data in the second example to set the true minimum to zero, over 99% of the bootstrap samples had a zero in them. \n",
                "\n",
                "In other words, this showed that in the second scenario, the minimum of any given sample was generally representative of the minimum of the true distribution. As such, we could be confident that a 1-point improvement was the true minimum.\n",
                "\n",
                "On the other hand, the first scenario showed that if there are only a few values at the true minimum, then much of the time a given sample will *not* contain the true minimum. Thus, in the first case, it's harder to know whether the true minimum is 1 or 0.\n",
                "\n",
                "Note especially that we cannot conclude from this that the true minimum is **not** 1. We just don't have much good evidence either way! We cannot reject the null hypothesis that just by chance our sample did not contain the true population minimum. But equally we cannot reject the alternative hypothesis that it did contain the true minimim! This is the fundamental limit of hypothesis testing with p-values.\n",
                "\n",
                "**One last note:** Even in the second scenario where we saw a significant improvement in scores, we can't attribute that improvement to the tutoring session! It's possible that maybe scores just go up as students have time to think more about a homework. (In this case, the better experiment would be a controlled trial, where students are randomized to go to the tutor session or not. We'll examine such two-sample tests in the homework for the next statistics lecture.) The main lesson of this is that even if you *can* reject the null hyopthesis, you can never be sure *which* alternative hypothesis is correct (i.e. whether tutoring helped, or some other factor improved the scores) unless you design an experiment to distinguish among the alternatives."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}