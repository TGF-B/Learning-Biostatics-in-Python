{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "3e8394a76db162f510df52a263868f8d",
                    "grade": false,
                    "grade_id": "cell-1650e3b0b3c7f222",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "# Question 0: Python Tools for Resampling\n",
                "\n",
                "In this question, we will develop general, higher-performance tools for resampling and bootstrap hypothesis testing using `numpy`.\n",
                "\n",
                "General-purpose tools are nice so that we won't have to keep re-writing functions to bootstrap the mean vs. the min, say.\n",
                "\n",
                "Performance is important in bootstrapping (and for the power analysis that will be the subject of the next lecture), because in these applications we will be simulating things tens of thousands of times over.\n",
                "\n",
                "Let's tackle performance first, using some tools from IPython to compare the speed of various operations using `numpy` vs. basic Python.\n",
                "\n",
                "A very useful IPython \"magic\" command for analyzing performance is `%timeit`. (Note for advanced users: \"magic\" commands are available in the Notebook, or by running `ipython` from the command-line, but not via the plain-vanilla `python` interpreter.)\n",
                "\n",
                "Try running the cell below to see how `%timeit` works. (Ignore any warnings about \"The slowest run took xx times longer than the fastest\": these are usually false positives, and certainly so in simple cases like these...)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "783ff0e0d501a7e2f1831d7b43d8faf0",
                    "grade": false,
                    "grade_id": "cell-b8b4aecb47128795",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "print('How long to make a list?')\n",
                "%timeit a = [1,2,3]\n",
                "print()\n",
                "\n",
                "print('How long to add numbers?')\n",
                "a = 5\n",
                "b = 6\n",
                "%timeit a + b\n",
                "print()\n",
                "\n",
                "print('How long for a simple loop?')\n",
                "def loop():\n",
                "    a = []\n",
                "    for i in range(100000):\n",
                "        a.append(i)\n",
                "\n",
                "%timeit loop()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "5159574a82dca28894fe276ab517ab36",
                    "grade": false,
                    "grade_id": "cell-582659c92d1fd206",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "As you can see, `%timeit` will carefully time fast things (that take nanoseconds) and slow things (taking milliseconds or longer).\n",
                "\n",
                "First, let's figure out the best way to resample data. Remember that resampling involves drawing a new dataset, the same size as the old one, with replacement. The python module `random` has a function `choices` that we previously used for this task, to draw elements with replacement from a list:\n",
                "```python\n",
                "import random\n",
                "a = [1,2,3,4,5,6]\n",
                "random.choices(a, k=len(a))\n",
                "```\n",
                "\n",
                "It happens that `numpy` also has a similar function, adapted to work not on lists, but on `numpy` arrays of numbers:\n",
                "```python\n",
                "import numpy\n",
                "a = numpy.array([1,2,3,4,5,6])\n",
                "numpy.random.choice(a, size=len(a), replace=True)\n",
                "```\n",
                "\n",
                "There are a few things to note:\n",
                "  1. `numpy.random.choice` can sample with or without replacement, controlled by the `replace` parameter.\n",
                "  2. The number of points to draw is controlled by the `size` parameter in `numpy.random.choice`, vs. the `k` parameter in `random.choices`.\n",
                "  3. `numpy` functions, such as `random.choice`, can take a list as input, and will internally convert that list into an array. But if you're going to do something a lot of times over, it makes more sense to convert the list to an array once at the beginning, rather than each time through...\n",
                " \n",
                "## Question 0.1\n",
                "In the cell below, use `%timeit` to compare the speed of three conditions: First, `random.choices` applied to a list; second, `numpy.random.choice` applied to a list; and third, `numpy.random.choice` applied to an array. Use the provided `data_list` and `data_array` below, and make sure to draw proper resamples of the right size, and with replacement.\n",
                "\n",
                "In the cell below that, write out which is the fastest, and approximately the fold-speedup between slowest and fastest (rounded to the nearest 5)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "a15c8f9f9ec270c3a6e580d9a328c9f7",
                    "grade": false,
                    "grade_id": "cell-c1da860f0025c6e2",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "import numpy\n",
                "import random\n",
                "data_list = list(range(1000))\n",
                "data_array = numpy.array(data_list)\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "# Note that the difference in speed between numpy.random.choice applied to a list\n",
                "# vs. to an array should be *approximately* the amount of time it takes to turn a\n",
                "# list into a numpy array. Test this:\n",
                "print('time to convert list to array:')\n",
                "%timeit data_array = numpy.array(data_list)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "23cbadd9ed44f62e54d63a19d529a731",
                    "grade": true,
                    "grade_id": "cell-019173190cb1a17d",
                    "locked": false,
                    "points": 1,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "source": [
                "YOUR ANSWER HERE"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A general performance rule is that \"if you are working with numbers, use numpy\". Another rule is that you should try to avoid looping through a `numpy` array with an explicit for loop. Instead, rely on `numpy` functions that operate on all the entries of the array at once.\n",
                "\n",
                "The function you wrote in the last stats homework to calcualte the fraction of data points more extreme (i.e. a p-value) is a good example of looping through a list of numbers to calculate something.\n",
                "\n",
                "## Question 0.2\n",
                "Write a similar function, below, called `p_value_from_list`. (In the next question, we'll make it work faster with `numpy`.) This function will take three parameters:\n",
                "  - `actual_stat`: the actual value of some statistic (like the mean of a dataset)\n",
                "  - `resampled_stats`: a list of values for the statistic after resampling *under the null hypothesis*.\n",
                "  - `null_hyp_stat`: the value of the statistic if the null hypothesis were true (often zero, which we will actually use as the default value of this parameter).\n",
                "  - `two_tailed`: if True (default), count differences as or more extreme in *either direction* from the null hypothesis. Otherwise only count differences as or more extreme in the same direction from the null hypothesis as `actual_stat`.\n",
                "\n",
                "Return the p-value. Note that you cannot assume that `actual_stat` is greater or less than `null_hyp_stat`. Make sure to handle all cases properly. For the two-tailed case, you will probably want to use the `abs` function to calculate the absolute value of the distance between `actual_stat` and `null_hyp_stat` and so forth.\n",
                "\n",
                "Note: it's better Python style to test if something like `two_tailed` is `True` as follows:\n",
                "```python\n",
                "if two_tailed:\n",
                "    do_something()\n",
                "```\n",
                "compared to the more redundant:\n",
                "```python\n",
                "if two_tailed == True:\n",
                "    do_something()\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "86230441e37e1f2f00a11e10f1647b73",
                    "grade": false,
                    "grade_id": "cell-da4d8e0ca5797091",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def p_value_from_list(actual_stat, resampled_stats, null_hyp_stat=0, two_tailed=True):\n",
                "    # YOUR ANSWER HERE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "2d8e3cbfcac31357f15eff73bf472fc9",
                    "grade": true,
                    "grade_id": "cell-f3aa9b92a94629a1",
                    "locked": true,
                    "points": 5,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# 100,000 numbers uniformly spaced from 0 to 1\n",
                "resampled_stats = list(numpy.linspace(0, 1, 100000))\n",
                "\n",
                "# 5% of the data should be as far or farther from 0.5 (in either direction) than is 0.975\n",
                "assert p_value_from_list(0.975, resampled_stats, null_hyp_stat=0.5) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0.5 (in the same direction) than is 0.975\n",
                "assert p_value_from_list(0.975, resampled_stats, null_hyp_stat=0.5, two_tailed=False) == 0.025\n",
                "\n",
                "# 5% of the data should be as far or farther from 0.5 (in either direction) than is 0.025\n",
                "assert p_value_from_list(0.025, resampled_stats, null_hyp_stat=0.5) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0.5 (in the same direction) than is 0.025\n",
                "assert p_value_from_list(0.025, resampled_stats, null_hyp_stat=0.5, two_tailed=False) == 0.025\n",
                "\n",
                "# 100,000 numbers uniformly spaced from -1 to 1\n",
                "resampled_stats = list(numpy.linspace(-1, 1, 100000))\n",
                "\n",
                "# 5% of the data should be as far or farther from 0 (in either direction) than is 0.95\n",
                "assert p_value_from_list(0.95, resampled_stats) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0 (in the same direction) than is 0.95\n",
                "assert p_value_from_list(0.95, resampled_stats, two_tailed=False) == 0.025\n",
                "\n",
                "# 5% of the data should be as far or farther from 0 (in either direction) than is -0.95\n",
                "assert p_value_from_list(-0.95, resampled_stats) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0 (in the same direction) than is -0.95\n",
                "assert p_value_from_list(-0.95, resampled_stats, two_tailed=False) == 0.025\n",
                "\n",
                "print('counting time:')\n",
                "%timeit p_value_from_list(0.975, resampled_stats, null_hyp_stat=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "74b719e417da01c342571bc2866db25b",
                    "grade": false,
                    "grade_id": "cell-51434757008ced55",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "One thing you can do with `numpy arrays` is adding two arrays together:\n",
                "```python\n",
                "a = numpy.array([1,2,3])\n",
                "b = numpy.array([2,4,6])\n",
                "a + b # is an array of [3, 6, 9]\n",
                "```\n",
                "As you can see (and test this out / play with similar commands in another notebook!), the addition proceeds element-wise: the first element in `a` is added to the first in `b`, and so forth. Multiplying, dividing, &c. work similarly.\n",
                "\n",
                "It turns out that numpy will also allow you to add/subtract/divide/whatnot a single number (i.e. a \"scalar\") to an array:\n",
                "```python\n",
                "a = numpy.array([1,2,3])\n",
                "a + 2 # is an array of [3, 4, 5]\n",
                "```\n",
                "\n",
                "One convenient feature of numpy is that you can *also* use comparisons in the same way:\n",
                "```python\n",
                "a = numpy.array([1,2,3])\n",
                "a > 2 # is an array of [False, False, True]\n",
                "```\n",
                "\n",
                "Now, how can we get a count of how many True values there are in an array? If you type `numpy.count` and then press TAB, to see what `numpy` functions have names starting with `count`, you will see exactly one: `numpy.count_nonzero`. Reading the documentation, you will see that what this function does is count how many elements are non-zero **or** not False. (Remember that `0` acts the same as `False` in many cases... `if 0:` is the same as `if False:` -- whatever is in the body of that if-statement will never run.)\n",
                "\n",
                "So:\n",
                "```python\n",
                "a = numpy.array([1,2,3])\n",
                "numpy.count_nonzero(a > 2) # returns 1\n",
                "numpy.count_nonzero(a >= 2) # returns 2\n",
                "```\n",
                "\n",
                "## Question 0.3\n",
                "Rewrite `p_value_from_list` as `p_value`, using addition/subtraction/comparisons of the all of the elements in the `resampled_stats` array at once, and `numpy.count_nonzero`. (There should be no for loops!) Note that you can use `numpy.abs` to get the absolute value of every element in an array.\n",
                "\n",
                "One issue: how do we know that `resampled_stats` is an array? If we do something like `a + 6`, we will get an error if `a` is `[1,2,3]`, but we will get what we want if `a` is `numpy.array([1,2,3])`. The standard approach to making sure that an input is always an array is to do this:\n",
                "```python\n",
                "def square_array(data):\n",
                "    data = numpy.asarray(data)\n",
                "    return data**2\n",
                "```\n",
                "\n",
                "What `numpy.asarray` does is (a) convert its input to an array if the input is not an array, or (b) return the input without doing anything if its input is an array.\n",
                "\n",
                "This differs slightly from `numpy.array`, which (a) converts its input to an array if the input is not an array, or (b) copies the input to a new array if the input is an array. (This is a lot like what the python fuction `list` does.)\n",
                "\n",
                "When dealing with large arrays, it can be slow to copy the contents to a new array. So generally `numpy.asarray` is the right approach.\n",
                "\n",
                "**Tip:** `arr.size` gives the number of elements in an array. (If you're sure that the array is 1-dimensional, this is equivalent to `len(arr)` or `arr.shape[0]`.) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "490a79849948f67f41251f51274efb48",
                    "grade": false,
                    "grade_id": "cell-c7d0ae011af6d981",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def p_value(actual_stat, resampled_stats, null_hyp_stat=0, two_tailed=True):\n",
                "    resampled_stats = numpy.asarray(resampled_stats)\n",
                "    # YOUR ANSWER HERE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "45929edb6b7b0942209364f6b1a17cd5",
                    "grade": true,
                    "grade_id": "cell-4593d1528b265d30",
                    "locked": true,
                    "points": 5,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# 100,000 numbers uniformly spaced from 0 to 1\n",
                "resampled_stats = numpy.linspace(0, 1, 100000)\n",
                "\n",
                "# 5% of the data should be as far or farther from 0.5 (in either direction) than is 0.975\n",
                "assert p_value(0.975, resampled_stats, null_hyp_stat=0.5) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0.5 (in the same direction) than is 0.975\n",
                "assert p_value(0.975, resampled_stats, null_hyp_stat=0.5, two_tailed=False) == 0.025\n",
                "\n",
                "# 5% of the data should be as far or farther from 0.5 (in either direction) than is 0.025\n",
                "assert p_value(0.025, resampled_stats, null_hyp_stat=0.5) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0.5 (in the same direction) than is 0.025\n",
                "assert p_value(0.025, resampled_stats, null_hyp_stat=0.5, two_tailed=False) == 0.025\n",
                "\n",
                "# 100,000 numbers uniformly spaced from -1 to 1\n",
                "resampled_stats = list(numpy.linspace(-1, 1, 100000))\n",
                "\n",
                "# 5% of the data should be as far or farther from 0 (in either direction) than is 0.95\n",
                "assert p_value(0.95, resampled_stats) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0 (in the same direction) than is 0.95\n",
                "assert p_value(0.95, resampled_stats, two_tailed=False) == 0.025\n",
                "\n",
                "# 5% of the data should be as far or farther from 0 (in either direction) than is -0.95\n",
                "assert p_value(-0.95, resampled_stats) == 0.05\n",
                "\n",
                "# 2.5% of the data should be as far or farther from 0 (in the same direction) than is -0.95\n",
                "assert p_value(-0.95, resampled_stats, two_tailed=False) == 0.025\n",
                "\n",
                "print('counting time:')\n",
                "%timeit p_value(0.975, resampled_stats, null_hyp_stat=0.5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "5cae0c2ccb47dbcaa38ad320f955b085",
                    "grade": false,
                    "grade_id": "cell-ab96453acc799e52",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So, the `numpy` version should be around 5-10\u00d7 faster. \n",
                "\n",
                "Though, the astute among you might notice that even the \"slow\" version should take only a handful of milliseconds to complete. You'd have to calculate thousands of p-values before you'd ever notice that the \"slow\" version is actually bothersome. Well, in the next problem set when doing power analysis, we will do just that!\n",
                "\n",
                "## Question 0.4\n",
                "\n",
                "Next, let's make a very general `resample` function. Previously we would have had to write different `resample_mean` and `resample_min` functions, even though absolutely everything about the function was the same except for calling the `mean` or `min` function:\n",
                "```python\n",
                "def mean(data):\n",
                "    return sum(data)/len(data)\n",
                "\n",
                "def resample_means(data, n_resamples):\n",
                "    means = []\n",
                "    for i in range(n_resamples):\n",
                "        resampled_data = random.choices(data, k=len(data))\n",
                "        means.append(mean(resampled_data))\n",
                "    return means\n",
                "\n",
                "def resample_mins(data, n_resamples):\n",
                "    mins = []\n",
                "    for i in range(n_resamples):\n",
                "        resampled_data = random.choices(data, k=len(data))\n",
                "        mins.append(min(resampled_data))\n",
                "    return mins\n",
                "```\n",
                "\n",
                "If you remember the last programming lecture, however, you know that you can store a function (either one that we wrote, like `mean`, or a Python built-in, like `min`) in a variable and pass that to another function. Rewrite the above into a generic `resample_1sample` function that can resample values for any statistic (i.e. any Python function) that is calculated from a single dataset (such as `min`, `mean`, &c.)\n",
                "\n",
                "Your `resample_1sample` function will take three parameters: `data` (which may be a list or a numpy array), `statistic` which will be any python function that takes a numpy array and returns a single number, and `n_resamples`, the number of resamples to perform. The function should then return a list of the values of calling `statistic` on resampled versions of the input data `n_resamples` different times.\n",
                "\n",
                "Note that even in a `numpy` world, it's still fine to incrementally build up a list in a for loop like the above functions do. There's no good way to avoid a for loop in this case. However, let's not use `random.choices` like in the functions above, given what we learned earlier in this question... And remember that `numpy.random.choice` works faster on a `numpy` array, so make sure to turn the data into a numpy array with `numpy.asarray` before doing anything else.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "46007dcf012514d8464a110822eaa6ca",
                    "grade": false,
                    "grade_id": "cell-a9a554ce3e1bdaab",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def resample_1sample(data, statistic, n_resamples):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "# simple usage example\n",
                "print(resample_1sample([1,2,3,4,5], min, n_resamples=10))\n",
                "print()\n",
                "\n",
                "# now let's compare speed\n",
                "def mean(data):\n",
                "    return sum(data) / len(data)\n",
                "\n",
                "# make up 1000 fake data points\n",
                "data = numpy.linspace(0, 10, 1000)\n",
                "\n",
                "def resample_means(data, n_resamples):\n",
                "    means = []\n",
                "    for i in range(n_resamples):\n",
                "        resampled_data = random.choices(data, k=len(data))\n",
                "        means.append(mean(resampled_data))\n",
                "    return means\n",
                "\n",
                "print('Time using resample_means:')\n",
                "%timeit resample_means(data, n_resamples=1000)\n",
                "print()\n",
                "\n",
                "print('Time using resample_1sample with the above mean function defined in python:')\n",
                "%timeit resample_1sample(data, mean, n_resamples=1000)\n",
                "print()\n",
                "\n",
                "print('Time using resample_1sample with numpy.mean, which is optimized for numpy arrays:')\n",
                "%timeit resample_1sample(data, numpy.mean, n_resamples=1000)\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "3319db51ee89ab9e913cd27f18c8640b",
                    "grade": true,
                    "grade_id": "cell-1a65520a456ad732",
                    "locked": true,
                    "points": 4,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "mins = resample_1sample([1,2,3,4,5], min, n_resamples=10000)\n",
                "# there is a (4/5)**5, or about 32.7%, chance that any given resample doesn't include 1.\n",
                "# So 32.7% of the time the min should be > 1. \n",
                "expected = 10000 * (4/5)**5\n",
                "actual = numpy.count_nonzero(numpy.array(mins) > 1)\n",
                "print(actual, expected)\n",
                "assert abs(actual - expected) < 200\n",
                "\n",
                "data = numpy.linspace(0, 10, 1000)\n",
                "means = resample_1sample(data, numpy.mean, n_resamples=10000)\n",
                "print(min(means), max(means))\n",
                "assert min(means) > 4.4\n",
                "assert max(means) < 5.5"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "b1b1cf86309c8ae4bfa61347039c6284",
                    "grade": false,
                    "grade_id": "cell-02a5353bc26421ec",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So, that's a generic 1-sample resampling function. But we also have lots of two-sample statistics we might be interested in! For example, the difference in means, or the t-statistic.\n",
                "\n",
                "Remember that the t-statistic is defined as:\n",
                "$$ t = k(n_1, n_2)\\cdot\\frac{\\mu_1 - \\mu_2}{\\sqrt{n_1\\cdot\\sigma^2_1 + n_2\\cdot\\sigma^2_2 }}$$\n",
                "\n",
                "where $\\mu_i$ is the mean of dataset i, $n_i$ is the number of elements in that dataset, and $\\sigma^2_i$ is its variance. As $k$ depends only on the $n_i$, it won't ever change within our resampling runs. Thus we will ignore it for our purposes. (It's critical for comparing t-values between experiments, but we don't need to do that here.)\n",
                "\n",
                "Let's define the difference in means as simply $\\mu_1 - \\mu_2$ (as opposed to $\\mu_2 - \\mu_1$).\n",
                "\n",
                "## Question 0.5\n",
                "Use the functions `numpy.mean` and `numpy.var` to write `mean_diff` and `t_stat` functions, and then write a `resample_2sample` function to work with them. Note that the difference in means and the t-statistic can be negative or positive.\n",
                "\n",
                "Unlike the `statistic` functions that we passed to `resample_1sample` above, which operate on a single array of data (like `numpy.mean` for example, which gets called as `numpy.mean(data)`), the functions we provide to `resample_2sample` will take two different arrays and will get called as e.g. `t_stat(data1, data2)`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "53328cbbfc70413c0c3bb23ded97848b",
                    "grade": false,
                    "grade_id": "cell-68c06f243b543d90",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def mean_difference(data1, data2):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "def t_stat(data1, data2):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "def resample_2sample(data1, data2, statistic, n_resamples):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "data1 = numpy.random.normal(loc=1, scale=0.1, size=10000)\n",
                "data2 = numpy.random.normal(loc=0, scale=0.1, size=10000)\n",
                "print(mean_difference(data1, data2), t_stat(data1, data2))\n",
                "\n",
                "data3 = numpy.random.normal(loc=1, scale=2, size=10000)\n",
                "data4 = numpy.random.normal(loc=0, scale=2, size=10000)\n",
                "print(mean_difference(data3, data4), t_stat(data3, data4))\n",
                "\n",
                "print(resample_2sample(data3, data4, t_stat, n_resamples=5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "41e5857b6f2f962c36b7f6332b157258",
                    "grade": true,
                    "grade_id": "cell-5314469b9667ef76",
                    "locked": true,
                    "points": 6,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert mean_difference([1,2,3], [4,5,6]) == -mean_difference([4,5,6], [1,2,3])\n",
                "assert mean_difference([1,2,3], [4,5,6]) == -3\n",
                "assert t_stat([1,2,3], [4,5,6]) == -1.5\n",
                "assert t_stat([0,2,4], [3,5,7]) == -0.75 # larger spread = smaller t\n",
                "assert t_stat([1,2,3], [7,8,9]) == -3 # larger difference in means = larger t\n",
                "\n",
                "mean_diffs = resample_2sample([1,1,1], [2,2,2], mean_difference, n_resamples=100)\n",
                "assert len(mean_diffs) == 100\n",
                "assert numpy.count_nonzero(numpy.array(mean_diffs) == -1) == 100\n",
                "\n",
                "data3 = numpy.random.normal(loc=1, scale=2, size=1000)\n",
                "data4 = numpy.random.normal(loc=0, scale=2, size=3000)\n",
                "t_stats = resample_2sample(data3, data4, t_stat, n_resamples=5000)\n",
                "print(min(t_stats), max(t_stats))\n",
                "assert min(t_stats) > 0.003\n",
                "assert max(t_stats) < 0.013"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "1391c6633aac8ac119c1af1f2a5a4145",
                    "grade": false,
                    "grade_id": "cell-aefc42a0b37aa024",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "You may have noticed that `resample_1sample` and `resample_2sample` looked pretty similar. And you can probably imagine that `resample_3sample` and `resample_4sample` would very much follow the same template. How could we make a totally-generic `resample` function that could take an arbitrary number of samples?\n",
                "\n",
                "To do this, we need to learn one special bit of syntax in Python. Recall list and tuple unpacking:\n",
                "```python\n",
                "a = [5, 6, 7]\n",
                "\n",
                "# list unpacking is convenient:\n",
                "first, middle, last = a\n",
                "\n",
                "# this is inconvenient:\n",
                "first = a[0]\n",
                "middle = a[1]\n",
                "last = a[2]\n",
                "```\n",
                "\n",
                "What if we had a three-parameter function, and a three-element list. Is there a way to \"unpack\" that list into the arguments of the parameters?\n",
                "```python\n",
                "def do_something(first, middle, last):\n",
                "    return (first + last) / middle\n",
                "\n",
                "a = [5, 6, 7]\n",
                "# can we do better than:\n",
                "do_something(a[0], a[1], a[2])\n",
                "\n",
                "# yes we can!\n",
                "do_something(*a)\n",
                "```\n",
                "\n",
                "That `*a` syntax means \"unpack the list `a` into the individual parameters that `do_something` requires. If `a` is too long or too short, an error would be raised, just as it would be if you did `do_something(1, 2)` or `do_something(1, 2, 3, 4)`.\n",
                "\n",
                "## Question 0.6\n",
                "Use this syntax to write a fully-general `resample` function, that takes a list of `data_sets` in addition to the statistic to use and the number of resamples. For each resampling run, resample each of the datasets, and then call the statistic on all of the resampled datasets. For example, each of the following should work:\n",
                "```python\n",
                "resample([data], numpy.mean, n_resamples=1000)\n",
                "resample([data1, data2], t_stat, n_resamples=1000)\n",
                "\n",
                "def three_sample_stat(data1, data2, data3):\n",
                "    # just silly\n",
                "    return numpy.mean(data1) - numpy.mean(data2) + numpy.var(data3)\n",
                "\n",
                "resample([data1, data2, data3], three_sample_stat, n_resamples=1000)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "6db2737ccca693370c80ceaed8d4dc66",
                    "grade": false,
                    "grade_id": "cell-bf425d3362ce9e07",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def resample(data_sets, statistic, n_resamples):\n",
                "    # turn every dataset in `data_sets` into a `numpy` array \n",
                "    data_sets = [numpy.asarray(data) for data in data_sets]\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "data1 = numpy.random.normal(loc=1, scale=0.1, size=1000)\n",
                "data2 = numpy.random.normal(loc=0, scale=0.1, size=1000)\n",
                "\n",
                "means = resample([data1], numpy.mean, n_resamples=1000)\n",
                "t_stats = resample([data1, data2], t_stat, n_resamples=1000)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "23b0ef0599a37cc7ee278b630e13e602",
                    "grade": true,
                    "grade_id": "cell-b68154e24e53d16b",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "mins = resample([[1,2,3,4,5]], min, n_resamples=10000)\n",
                "# there is a (4/5)**5, or about 32.7%, chance that any given resample doesn't include 1.\n",
                "# So 32.7% of the time the min should be > 1. \n",
                "expected = 10000 * (4/5)**5\n",
                "actual = numpy.count_nonzero(numpy.array(mins) > 1)\n",
                "assert abs(actual - expected) < 200\n",
                "\n",
                "data = numpy.linspace(0, 10, 1000)\n",
                "means = resample([data], numpy.mean, n_resamples=10000)\n",
                "assert min(means) > 4.4\n",
                "assert max(means) < 5.5\n",
                "\n",
                "mean_diffs = resample([[1,1,1], [2,2,2]], mean_difference, n_resamples=100)\n",
                "assert len(mean_diffs) == 100\n",
                "assert numpy.count_nonzero(numpy.array(mean_diffs) == -1) == 100\n",
                "\n",
                "data3 = numpy.random.normal(loc=1, scale=2, size=1000)\n",
                "data4 = numpy.random.normal(loc=0, scale=2, size=3000)\n",
                "t_stats = resample([data3, data4], t_stat, n_resamples=5000)\n",
                "assert min(t_stats) > 0.003\n",
                "assert max(t_stats) < 0.013"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "e73f55cc8675733c65068a290abac8c9",
                    "grade": false,
                    "grade_id": "cell-eb5f4fda76f43e2d",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Now we know how to call a function with a variable number of arguments: just put them into a list and use the `*` syntax:\n",
                "```python\n",
                "a = [1, 2, 3]\n",
                "print(*a)\n",
                "```\n",
                "\n",
                "Note also that some functions, such as `print` above, can *receive* a variable number of arguments! `print('hello')` works, as does `print('hello', 2, 'goodbye')`, and so forth. As yet, we don't know how to write such a function. Fortunately, the `*` syntax is even more versatile, and can be used to define a function that takes zero or more arguments:\n",
                "```python\n",
                "def var_args(*args):\n",
                "    print(type(args), len(args))\n",
                "    print(args)\n",
                "\n",
                "var_args()\n",
                "# prints:\n",
                "# <class 'tuple'> 0\n",
                "# ()\n",
                "\n",
                "var_args(1)\n",
                "# prints:\n",
                "# <class 'tuple'> 1\n",
                "# (1,)\n",
                "\n",
                "var_args(1, 2, 3)\n",
                "# prints:\n",
                "# <class 'tuple'> 3\n",
                "# (1, 2, 3)\n",
                "\n",
                "a = [1, 2, 3]\n",
                "var_args(*a)\n",
                "# prints:\n",
                "# <class 'tuple'> 3\n",
                "# (1, 2, 3)\n",
                "```\n",
                "\n",
                "So, as you can see, defining a function that takes a `*` argument (which can be named anything, though `*args` is traditional) allows that function to be called with a variable number of arguments. Inside the function, the arguments are all put into a single tuple containing the arguments the function was called with.\n",
                "\n",
                "In class, we talked about a statistic that can work with an arbitrary number of data points: the F-statistic, which drives ANOVA. For a given number of data groups, the F-statistic concerns the relationship between three sums:\n",
                "1. The total sum of squared differences\n",
                "$$ SS_{\\mathrm{total}} = \\sum_i \\sum_j (\\mathrm{data}_{ij} - \\mu)^2 $$\n",
                "2. The within-group sum of squared differences\n",
                "$$ SS_{\\mathrm{within}} = \\sum_i \\sum_j (\\mathrm{data}_{ij} - \\mu_i)^2 $$\n",
                "3. The between group sum of squared differences\n",
                "$$ SS_{\\mathrm{between}} = \\sum_i n_i \\cdot (\\mu_i - \\mu)^2 $$\n",
                "Where:\n",
                "  - the indices *j* refer to the different data points in each group *i*\n",
                "  - $data_{ij}$ is the *jth* element of group *i*\n",
                "  - $\\mu_i$ is the mean of data group *i*\n",
                "  - $n_i$ number of data points in group *i*\n",
                "  - $\\mu$ is the grand mean of all the data points together. \n",
                "\n",
                "Recall that these sums of squares are related as: $S_{\\mathrm{total}} = SS_{\\mathrm{within}} + SS_{\\mathrm{between}}$.\n",
                "\n",
                "The F-statistic is:\n",
                "$$ F = \\frac{SS_{\\mathrm{between}}}{SS_{\\mathrm{within}}} $$\n",
                "\n",
                "## Question 0.7\n",
                "Below, a function called `sum_of_squares` is defined that returns the sum of squared distances between each point in a dataset and a given reference value (i.e. the \"reference\" could be $\\mu_i$ or $\\mu$ above).\n",
                "\n",
                "Write two functions. The first, `all_sums`, we will use to test that the sums of squares add as we expect: it should take an arbitrary number of data sets and return three values: the total, within-group, and between-group sums of squares, each calculated as described above. Note that to calculate the grand mean, you will need a single array consisting of all the data sets \"stuck together\". (This array can simplify calculating the total sum of squares too.) We can do that with `numpy.concatenate`, which literally means \"to stick together end-to-end\". \n",
                "\n",
                "The second, `F-stat`, should share a lot of similarity with `all_sums`, but it should just calculate and return the ratio of the between- to within-group sum of squares."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "70ec36591a312498cc8bc4d2c5b75bc0",
                    "grade": false,
                    "grade_id": "cell-4ffde7047e3f1128",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def sum_of_squares(data, reference):\n",
                "    data = numpy.asarray(data)\n",
                "    # look: no for loop! We let numpy do everything for us. Much faster that way, and simpler to read.\n",
                "    return numpy.sum((data - reference)**2)\n",
                "\n",
                "def all_sums(*data_sets):\n",
                "    # turn every dataset in `data_sets` into a `numpy` array \n",
                "    data_sets = [numpy.asarray(data) for data in data_sets]\n",
                "    all_data = numpy.concatenate(data_sets)\n",
                "    grand_mean = numpy.mean(all_data)\n",
                "    # YOUR ANSWER HERE\n",
                "    return ss_total, ss_within, ss_between\n",
                "\n",
                "data1 = numpy.random.normal(loc=0, scale=2, size=300)\n",
                "data2 = numpy.random.normal(loc=1, scale=2, size=500)\n",
                "data3 = numpy.random.normal(loc=2, scale=2, size=700)\n",
                "\n",
                "ss_total, ss_within, ss_between = all_sums(data1, data2, data3)\n",
                "print(ss_total, ss_within + ss_between)\n",
                "\n",
                "def F_stat(*data_sets):\n",
                "    # YOUR ANSWER HERE\n",
                "    return ss_between / ss_within\n",
                "\n",
                "print(F_stat(data1, data2, data3))\n",
                "\n",
                "data4 = numpy.random.normal(loc=0, scale=1, size=300)\n",
                "data5 = numpy.random.normal(loc=1, scale=1, size=500)\n",
                "data6 = numpy.random.normal(loc=2, scale=1, size=700)\n",
                "# F-stat should get bigger as the spread within each group decreases\n",
                "# Here we go from \"scale\" (i.e. standard deviation) = 2 in data 1-3 to 1 in data 4-6.\n",
                "print(F_stat(data4, data5, data6))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "307066665f96d87f8c624793c040f794",
                    "grade": true,
                    "grade_id": "cell-7aa528b2ac9e6807",
                    "locked": true,
                    "points": 4,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "data1 = numpy.random.normal(loc=0, scale=2, size=300)\n",
                "data2 = numpy.random.normal(loc=1, scale=2, size=500)\n",
                "data3 = numpy.random.normal(loc=2, scale=2, size=700)\n",
                "ss_total, ss_within, ss_between = all_sums(data1, data2, data3)\n",
                "assert abs(ss_total - (ss_within + ss_between)) < 0.0000001\n",
                "\n",
                "F_stats = resample([data1, data2, data3], F_stat, n_resamples=3000)\n",
                "print(min(F_stats), F_stat(data1, data2, data3), max(F_stats))\n",
                "assert min(F_stats) > 0.04\n",
                "assert max(F_stats) < 0.32"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "7fb664eedf63343d285945a46c127b71",
                    "grade": false,
                    "grade_id": "cell-56c8a1cef2564ced",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "In class, I said that the F-statistic is just the square of the t-statistic. This is exactly true, if you take into account the defintion of $k(n_1, n_2)$ that we ignored above. If we ignore $k$, then the F-statistic is simply proportional to the square of the t-statistic. Run the code below to generate F- and t-statistics on data with a range of differences in mean, and plot their relationship:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "cbccabfb58c17e30bec4a6f5ff09e28a",
                    "grade": false,
                    "grade_id": "cell-ac2365b21c3d36cb",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "from matplotlib import pyplot as plt\n",
                "plt.style.use('ggplot')\n",
                "\n",
                "Fs = []\n",
                "ts = []\n",
                "data1 = numpy.random.normal(loc=0, scale=2, size=300)\n",
                "for center in numpy.linspace(-4, 4, 500):\n",
                "    data2 = numpy.random.normal(loc=center, scale=2, size=500)\n",
                "    Fs.append(F_stat(data1, data2))\n",
                "    ts.append(t_stat(data1, data2))\n",
                "    \n",
                "plt.scatter(ts, Fs)\n",
                "plt.xlabel('t-statistic')\n",
                "plt.ylabel('F-statistic')\n",
                "print('looks quadradic indeed!')"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}