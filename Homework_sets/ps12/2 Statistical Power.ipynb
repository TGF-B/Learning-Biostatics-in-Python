{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "8f10fdd5014d80ef6dc6c6dbe6bfbcf9",
                    "grade": false,
                    "grade_id": "cell-0f32440856ba08a5",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "# Question 2: Statistical Power\n",
                "\n",
                "We will now explore statistical power. First, definitions:\n",
                "\n",
                "1. The p-value controls the *false positive rate*: the chance that we declare there to be a true effect when really what we saw was due to chance alone. (This is called a **\"Type I Error\"**.) Our p-value cutoff (typically 0.05 in biology) is the false positive rate that we are willing to tolerate.\n",
                "\n",
                "2. Statistical power is the *true positive rate* (or equivalently, 1 minus the false negative rate): the chance that when there really is an effect, we will get a p-value below the cutoff. Power depends on, amongst other things, (a) the size of the effect, (b) the size of the sample, and (c) the p-value cutoff. (Increasing the p-value cutoff will improve statistical power, but at the cost of also increasing the false positive rate.) Getting a false negative (which is more common when a statistical test has low power) is called a **\"Type II Error\"**.\n",
                "\n",
                "As a warm-up, let's first explore the false positive / type I error rate. We will do so by examining how often we see \"significant\" p-values from data that we know come from identical populations.\n",
                "\n",
                "To do this, we need to repeatedly generate pairs of \"new\" datasets drawn from the same underlying population. We can do this with the random number generators in python, or we can test resamples from the same data set for differences.\n",
                "\n",
                "**Crucially**, in neither case should we repeatedly apply a statistical test to the same data set over and over again. We want to know how often a pair of samples will be \"significantly\" far apart (which requires drawing new samples each time), **not** how often a test gives a significant p-value for any specific pair of samples.\n",
                "\n",
                "## Question 2.1\n",
                "Some code that's pretty familiar from the previous questions is provided below. (Note that the implementation of `bootstrap_means_different` has a couple changes: it no longer takes a `statistic` parameter, and instead always just calls `mean_difference` as the statistic. It also only returns the p-value.)\n",
                "\n",
                "We will use these to test whether the bootstrap test of two means gives the correct number of false positives given a specific p-value threshold. Write a function, `significant_fraction(test, get_sample, sample_size, n_trials=250, n_boot_resamples=1000, p_thresh=0.05)` to run this test:\n",
                "  - `test` is a function that implements a statistical test. Assume that the \"function signature\" of test (i.e. what parameters it takes) is `test(data_sets, n_resamples)`. I.e. we will pass `bootstrap_means_different` function above as a test of interest. (We will write and use other tests that have the same function signature later on.)\n",
                "  - `get_sample` is a function that will be called as `get_sample(sample_size)` and will return a new sample. (Just as in Question 0.5.)\n",
                "  - `sample_size` is the sample-size parameter to pass to `get_sample`.\n",
                "  - `n_trials` is the number of times to run the statistical test.\n",
                "  - `n_boot_resamples` is the number of resamples parameter to pass to `test`.\n",
                "  - `p_thresh` is a p-value threshold.\n",
                "\n",
                "For each trial (of the `n_trials` total trials), the `significant_fraction` should call `get_sample` **twice** to generate two samples from the same \"true distribution\" (i.e. the true difference in means is zero). Then the function should run the given statistical test with those two samples. The function should return the fraction of trials for which the p-value was less than or equal to the provided threshold."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "7d5e59f5339203018433052593763d4e",
                    "grade": false,
                    "grade_id": "cell-7470cda95debe56d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "import numpy\n",
                "\n",
                "aa_bp = []\n",
                "gg_bp = []\n",
                "file = open('bloodpressure3.txt')\n",
                "header = file.readline()\n",
                "for line in file:\n",
                "    bp, genotype = line.strip('\\n').split('\\t')\n",
                "    bp = float(bp)\n",
                "    if genotype == 'AA':\n",
                "        aa_bp.append(bp)\n",
                "    elif genotype == 'GG':\n",
                "        gg_bp.append(bp)\n",
                "    else:\n",
                "        print('unknown genotype!', genotype)\n",
                "file.close()\n",
                "aa_bp = numpy.array(aa_bp)\n",
                "gg_bp = numpy.array(gg_bp)\n",
                "\n",
                "def mean_difference(data1, data2):\n",
                "    return numpy.mean(data1) - numpy.mean(data2)\n",
                "\n",
                "def resample(data_sets, statistic, n_resamples):\n",
                "    data_sets = [numpy.asarray(data_set) for data_set in data_sets]\n",
                "    stats = []\n",
                "    for i in range(n_resamples):\n",
                "        resampled_data_sets = [numpy.random.choice(data, size=len(data), replace=True) for data in data_sets]\n",
                "        stats.append(statistic(*resampled_data_sets))\n",
                "    return stats\n",
                "\n",
                "def two_tail_p_value(actual_stat, resampled_stats, null_hyp_stat=0):\n",
                "    resampled_stats = numpy.asarray(resampled_stats)\n",
                "    actual_diff = abs(actual_stat - null_hyp_stat)\n",
                "    diff = numpy.abs(resampled_stats - null_hyp_stat)\n",
                "    count_extreme = numpy.count_nonzero(diff >= actual_diff)\n",
                "    return count_extreme / len(resampled_stats)\n",
                "\n",
                "def bootstrap_means_different(data_sets, n_resamples=10000):\n",
                "    data_sets = [numpy.asarray(data_set) for data_set in data_sets]\n",
                "    actual_stat = mean_difference(*data_sets)\n",
                "    shifted = [data_set - numpy.mean(data_set) for data_set in data_sets]\n",
                "    resampled_stats = resample(shifted, mean_difference, n_resamples)\n",
                "    p_val = two_tail_p_value(actual_stat, resampled_stats)\n",
                "    return p_val\n",
                "\n",
                "def get_normal_sample(sample_size):\n",
                "    # draw data from a normal distribution with mean=0 and std=1\n",
                "    return numpy.random.normal(size=sample_size)\n",
                "\n",
                "def get_gg_sample(sample_size):\n",
                "    # draw a re-sample of the GG data.\n",
                "    return numpy.random.choice(gg_bp, size=sample_size)\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "false_pos_norm = significant_fraction(bootstrap_means_different, get_normal_sample, sample_size=100)\n",
                "print(false_pos_norm)\n",
                "\n",
                "false_pos_gg = significant_fraction(bootstrap_means_different, get_gg_sample, sample_size=len(gg_bp))\n",
                "print(false_pos_gg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4d6671e49f6c8675c2e4341834303d7a",
                    "grade": true,
                    "grade_id": "cell-ebd9d7811a1dbb60",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.02 < false_pos_norm < 0.08\n",
                "assert 0.02 < false_pos_gg < 0.08\n",
                "num_false_pos = false_pos_gg * 250\n",
                "assert int(num_false_pos) == num_false_pos"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "f48cf8bfa79e1b641928fa58c070af0b",
                    "grade": false,
                    "grade_id": "cell-470af59ce3b6e6f4",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "You should see that the false positive rate is somewhere near 5%, which is as it should be for a p-value threshold of 0.05.\n",
                "\n",
                "## Question 2.2\n",
                "Recall from Question 0.8 that we didn't always get the \"right\" confidence interval for the median, compared to the mean. Essentially the problem was that resampling a specific dataset gave a distribution of medians that wasn't quite the same as you would see by sampling a truly new dataset over and over again.\n",
                "\n",
                "Specifically, the distribution of resamples was a tad too narrow, leading to confidence intervals that were too small. One might also suspect that a too-narrow distribution might lead to p-values that are smaller than maybe they ought to be. Is this a cause for concern? Let's test. Below, paste your code for `median_difference` and `bootstrap_medians_different` from Question 1.2. Modify `bootstrap_medians_different` so that it does not take a `statistic` parameter and instead always just uses `median_difference`. Also modify the function to only return the p-value. This way, it will be compatible with your `significant_fraction` function above.\n",
                "\n",
                "Running the code will answer the question of whether we get too many false positives for difference-of-medians tests.\n",
                "\n",
                "Note that we only test this with the `get_normal_sample` function, which returns a uniquely new sample from the normal distribution. In particular, since `get_gg_sample` \"cheats\" and returns a *resample* of the GG data (instead of getting a new population of individuals, genotyping them, and measuring their blood pressure!) we can't use it to ask whether *resamples* act just like *new samples* in this case."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "e86ec2b907037a8c981ae6b460eff202",
                    "grade": false,
                    "grade_id": "cell-6b5f3d30689c5709",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "# YOUR ANSWER HERE\n",
                "false_pos_norm_median = significant_fraction(bootstrap_medians_different, get_normal_sample, sample_size=100)\n",
                "print(false_pos_norm_median)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4437f2b6409859bfa4ee2a7642929b08",
                    "grade": true,
                    "grade_id": "cell-535f0a6f5b63f13f",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.02 < false_pos_norm < 0.08"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "9cb5d34e6d93ecddea36f77f7cd798bb",
                    "grade": false,
                    "grade_id": "cell-019a9500f86b9bc9",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Good news! The false-positive rate isn't too bad. Clearly, even if the distribution of medians of resampled statistics is a bit wonky, the distribution of *differences between two medians* isn't that different for resamples or true new samples.\n",
                "\n",
                "Run the code below to see that. This is just like the example at the end of Question 0, except that wer're looking at differences between medians rather than medians themselves. (Again, note that the overall positions of the distributions might differ, but that in this case the shapes are more similar than in Question 0.\n",
                "\n",
                "This is an illustration of the *[central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)*, one of the crown jewels of statistical theory. This theorem states that sume (or differences) of random values will typically have a normal (bell-curve-like) distribution, even when the underlying random variables come from distributions with very different shapes. (Sums of larger numbers of independent random variables will be more normally distributed than sums of fewer numbers, but as we see in this case, even two numbers added looks a lot smoother than the underlying distribution.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "8b8c07802c3df754510e7be04ec3220b",
                    "grade": false,
                    "grade_id": "cell-df45e63a1834bda0",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "median_diffs = [median_difference(get_normal_sample(200), get_normal_sample(200)) for i in range(10000)]\n",
                "\n",
                "sample1 = get_normal_sample(200)\n",
                "sample2 = get_normal_sample(200)\n",
                "resampled_median_diffs = resample([sample1, sample2], median_difference, n_resamples=10000)\n",
                "\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('ggplot')\n",
                "plt.rcParams['figure.figsize'] = [12, 4]\n",
                "\n",
                "plt.hist(median_diffs, bins='auto', label='sample median diffs')\n",
                "plt.hist(resampled_median_diffs, bins='auto', alpha=0.6, label='resample median diffss')\n",
                "x = plt.legend() # store result of this to suppress printing it out"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": true,
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "8b6bdbdfa8960c2e1e6d638f43513d32",
                    "grade": false,
                    "grade_id": "cell-19557b829df70757",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Next, let's compute statistical power: the probablility of observing a statistically significant p-value, given a particular size of effect, a particular sample size, a statistical test, and a p-value threshold.\n",
                "\n",
                "We want to write a function called `power` which will run any given hypothesis test. However, some tests might need different function arguments (e.g. a bootstrap or permutation test might require `n_resamples`, but a parametric Student's t-Test won't...) How can our `power` function accept arbitrary keyword arguments to pass along to the statistical test?\n",
                "\n",
                "Last week, we learned about the `*` syntax, whereby a function could accept an arbitrary list of parameters, or could call another function with such a list. There is another similar syntax for accepting arbitrary keyword arguments: `**`. Try running the below, and maybe changing it around a bit to see what happens:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "c6c0ad9bf37b9277dbb56ab929f9404a",
                    "grade": false,
                    "grade_id": "cell-ef70f69a18aa4d8f",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "def print_kws(**kws):\n",
                "    # kws is a dictionary of the keyword arguments provided to the function.\n",
                "    # kws maps keyword names (as strings) to values\n",
                "    for name, value in kws.items():\n",
                "        print(name, value)\n",
                "\n",
                "print_kws(hello=5, goodnight='moon')\n",
                "print_kws(a=4, b=5, c=5)\n",
                "# The below will generate an error if you uncomment  and run it (try it)!\n",
                "# Note that \"positional\" arguments (as refered to by the error text) are \n",
                "# arguments not specified by keyword, like those below:\n",
                "# print_kws(2, 4, 5)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "fd00a075a0ebe727471968c8661c32d3",
                    "grade": false,
                    "grade_id": "cell-5ebc6627d6a8e57b",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "In addition, we can call a function with an arbitrary dictionary of keyword arguments with `**`. Note that you can also provide some number of positional and keyword arguments before the `**` syntax. It's only an error to not provide enough arguments, or to provide too many or overlapping arguments. Again, try running (and playing with) the below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "49c9aa5b016d2b117880b476a4829844",
                    "grade": false,
                    "grade_id": "cell-1e3b489b46b20e2a",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "def many_args(a, b, c, d=5):\n",
                "    print('a', a)\n",
                "    print('b', b)\n",
                "    print('c', c)\n",
                "    print('d', d)\n",
                "    print()\n",
                "\n",
                "# can use all positional arguments\n",
                "many_args(1, 2, 3, 4)\n",
                "\n",
                "# equivalently, we can use a list with the * syntax:\n",
                "args = [1, 2, 3, 4]\n",
                "many_args(*args)\n",
                "\n",
                "# can use all keyword arguments\n",
                "many_args(a=1, b=2, c=3, d=4)\n",
                "\n",
                "# equivalently, can use a dictionary with the ** syntax, mapping strings to argument values:\n",
                "kws = {'a':1, 'b':2, 'c':3, 'd':4}\n",
                "many_args(**kws)\n",
                "\n",
                "# we can mix these patterns ad nauseum:\n",
                "\n",
                "kws = {'b':2, 'c':3}\n",
                "many_args(a=1, **kws) # note that we don't specify d at all! This gives us its default value of 5.\n",
                "many_args(4, d='hello', **kws) # specify a by position and d by explicit keyword\n",
                "\n",
                "args = [1, 2]\n",
                "kws = {'c': 3, 'd':4}\n",
                "many_args(*args, **kws)\n",
                "many_args(*args, c=3) # just use default value for d\n",
                "many_args(*args, c=3, d=4)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Last, note that while it's traditional to name the `**` parameter `**kws` or `**kwargs` (and use `*args` for the counterpart syntax), this isn't required in the slightest. Any variable name can be used (as you'll see below).\n",
                "\n",
                "## Question 2.3\n",
                "Write a function `power(test, get_sample, effect_size, sample_size, n_trials=250, p_thresh=0.05, **test_args)` that will repeatedly get new samples and run a two-sample test on those samples, where:\n",
                " - `test` is a function that can be called as `test(data_sets, **test_args)`, where `data_sets` is a list of two new samples. This function can be assumed to return a p-value and nothing else.\n",
                " - `get_sample` is a function that can be called as `get_sample(sample_size, effect_size)` and will return a new sample of the given size and degree of \"effect\". `get_sample(sample_size, 0)` should return a no-effect \"control\" sample.\n",
                " - `effect_size` and `sample_size` are the parameters to `get_sample` described above.\n",
                " - `n_trials` is the number of times that new samples should be generated and the test run.\n",
                " - `p_thresh` is the threshold for calling a result significant\n",
                " - `**test_args` are arguments to pass to the `test` function.\n",
                "\n",
                "This function should return the fraction of `n_trials` where `test` returns a p-value that is less than or equal to `p_thresh`.\n",
                "\n",
                "The function should call `get_sample` twice to generate two separate datasets: once as `get_sample(sample_size, 0)` to get the \"control\" sample, and once as `get_sample(sample_size, effect_size)` to generate a sample that is distinct from the control by some specified effect size. (Usually this will amount to shifting the mean by the specified effect size...)\n",
                "\n",
                "Some potential `get_sample` functions are defined below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "d21e7624c3d945ee6701908ba8231a74",
                    "grade": false,
                    "grade_id": "cell-fa366fd794af6e72",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def power(test, get_sample, effect_size, sample_size, n_trials=250, p_thresh=0.05, **test_args):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "# define some sample-getting tools. Each of these returns samples with a standard deviation\n",
                "# around 12 (which is that of the GG data), so that the \"effect size\" parameters mean the same\n",
                "# thing for each. (Recall that a \"big effect\" is always relative to how variable the data are.)\n",
                "def get_normal_sample(sample_size, effect_size):\n",
                "    # note: loc = \"mean\" and scale = \"standard deviation\"\n",
                "    return numpy.random.normal(size=sample_size, loc=effect_size, scale=12)\n",
                "\n",
                "def get_gg_sample(sample_size, effect_size):\n",
                "    # draw a re-sample of the GG data, translated by effect_size units.\n",
                "    return numpy.random.choice(gg_bp + effect_size, size=sample_size)\n",
                "\n",
                "def get_bimodal_sample(sample_size, effect_size):\n",
                "    # draw a sample from a bimodal distribution\n",
                "    large_mode_size = int(sample_size * 2/3)\n",
                "    small_mode_size = sample_size - large_mode_size\n",
                "    large_mode = numpy.random.normal(size=large_mode_size, loc=effect_size, scale=7)\n",
                "    small_mode = numpy.random.normal(size=small_mode_size, loc=22+effect_size, scale=6)\n",
                "    return numpy.concatenate([large_mode, small_mode])\n",
                "\n",
                "plt.hist(get_normal_sample(10000, 0), bins='auto')\n",
                "plt.title('normal distribution')\n",
                "plt.figure()\n",
                "print('normal std:', numpy.std(get_normal_sample(10000, 0)))\n",
                "\n",
                "plt.hist(get_bimodal_sample(10000, 0), bins='auto')\n",
                "plt.title('bimodal distribution')\n",
                "plt.figure()\n",
                "print('bimodal std:', numpy.std(get_bimodal_sample(10000, 0)))\n",
                "\n",
                "plt.hist(gg_bp, bins='auto')\n",
                "plt.title('GG distribution')\n",
                "plt.figure()\n",
                "print('GG std:', numpy.std(gg_bp))\n",
                "print()\n",
                "\n",
                "# test the power of our bootstrap_means_different test and a normal sample.\n",
                "# note that the 'n_resamples' parameter, because it is not defined explicitly in the declaration of the\n",
                "# 'power' function, gets stuffed into the '**test_args' parameter, and thus passed through to the \n",
                "# 'bootstrap_means_different' function. (If you look at that function, note that it does in fact define\n",
                "# such a parameter...)\n",
                "norm_p = power(bootstrap_means_different, get_normal_sample, effect_size=4, sample_size=100, n_resamples=1000)\n",
                "print('normal distribution power:', norm_p)\n",
                "\n",
                "bimodal_p = power(bootstrap_means_different, get_bimodal_sample, effect_size=4, sample_size=100, n_resamples=1000)\n",
                "print('bimodal distribution power:', bimodal_p)\n",
                "\n",
                "gg_p = power(bootstrap_means_different, get_gg_sample, effect_size=4, sample_size=100, n_resamples=1000)\n",
                "print('GG distribution power:', gg_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "3b8d01bb6be26cdd40eefff9057841a7",
                    "grade": true,
                    "grade_id": "cell-9ab116a8fa5e40b3",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "# make sure that 250 trials were performed\n",
                "assert (norm_p * 250) == int(norm_p * 250)\n",
                "assert abs(norm_p - 0.68) < 0.1\n",
                "assert abs(bimodal_p - 0.7) < 0.1\n",
                "assert abs(gg_p - 0.62) < 0.1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "ca0fc2f1e4337151c7b23338a496ffa1",
                    "grade": false,
                    "grade_id": "cell-2b7fa128e13f28f7",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "From this, you should see that with a 4-unit difference between two groups (each with a standard deviation around 12) and a sample size of 100, there is about a 65% chance of getting a \"significant\" p-value. There may be some slight differences between the distributions, but if you re-run the above a few times the numbers bounce around a lot. (Something like 500+ trials and 5-10,000 resamples are necessary to start reliably seeing these small differences, but that's pretty slow.)\n",
                "\n",
                "## Question 2.4\n",
                "Let's now test the relationship between power and sample size. Using the bimodal sample and the `bootstrap_means_different` test (with 1000 resamples), calculate the power for each of the provided sample sizes and store the results in list titled `powers`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "83d082d7ee9093c85d0060cec5d887cd",
                    "grade": false,
                    "grade_id": "cell-8d34cff4fcdb7131",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "sample_sizes = [10, 40, 80]\n",
                "effect_size = 6\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "plt.plot(sample_sizes, powers)\n",
                "print(powers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "97dc50f167fbcc68baf7beaea19eeec1",
                    "grade": true,
                    "grade_id": "cell-9f3e1afe835cceef",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert len(powers) == 3\n",
                "assert powers[2] > powers[1] > powers[0]\n",
                "assert powers[0] < 0.15\n",
                "assert powers[2] > 0.85"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "5c5e22ce42c215b764d2f22418dc43ff",
                    "grade": false,
                    "grade_id": "cell-7babb7ff2f12d232",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So, as expected, power increases with sample size: with 10 samples, you have less than a 10% chance of getting a positive result when the difference between means is about half of the standard deviation of the data (which is a pretty big effect). But with 80 samples, you are almost certain to get a positive result.\n",
                "\n",
                "You could plot a similar curve for effect size as well, of course, or for any particular property of your data or statistical test.\n",
                "\n",
                "## Question 2.5\n",
                "What about the specific statistic used? Can we compare the t-statistic to the plain old difference in means?\n",
                "\n",
                "Out previous `bootstrap_means_different` function was hard-coded to use only the mean-difference statistic. Previously we had defined a more-general bootstrap function, a simplified version of which is below as `bootstrap_p`. This function requires a statistic to be provided in addition to the `data_sets` and `n_resamples` parameters.\n",
                "\n",
                "Can you use your above `power` function with `bootstrap_p` to compare the power of using the `mean_difference` statistic to that of the `t_stat` statistic? This shouldn't require editing any function definitions: you just have to call `power` in the right way to provide `bootstrap_p` with all the parameters it needs.\n",
                "\n",
                "Use an effect size of 6, a sample size of 20, the default 250 trials, 1000 bootstrap resamples and `get_normal_sample`. Store the results in `mean_diff_p` and `t_stat_p`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "d93a327419ff6e11cf8f259f69c2264f",
                    "grade": false,
                    "grade_id": "cell-fcf8e27f4aee2a5f",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def bootstrap_p(data_sets, statistic, n_resamples=10000):\n",
                "    data_sets = [numpy.asarray(data_set) for data_set in data_sets]\n",
                "    actual_stat = statistic(*data_sets)\n",
                "    shifted = [data_set - numpy.mean(data_set) for data_set in data_sets]\n",
                "    resampled_stats = resample(shifted, statistic, n_resamples)\n",
                "    p_val = two_tail_p_value(actual_stat, resampled_stats)\n",
                "    return p_val\n",
                "\n",
                "def t_stat(data1, data2):\n",
                "    # Here we use the data several times, so it makes sense to convert to arrays just once beforehand,\n",
                "    # rather than having numpy do it for us each time.\n",
                "    data1 = numpy.asarray(data1)\n",
                "    data2 = numpy.asarray(data2)\n",
                "    numerator = numpy.mean(data1) - numpy.mean(data2)\n",
                "    denominator = (len(data1) * numpy.var(data1) + len(data2) * numpy.var(data2))**0.5\n",
                "    return  numerator / denominator \n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "print('mean diff power:', mean_diff_p)\n",
                "print('t statistic power:', t_stat_p)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "66e3462e3bd727c980e1ff642ff1b6c6",
                    "grade": true,
                    "grade_id": "cell-426546d71ca5603b",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert abs(mean_diff_p - t_stat_p) < 0.2\n",
                "assert abs(mean_diff_p - 0.35) < 0.1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "bf8b8f6b13b3fa7941a955762c80f7a7",
                    "grade": false,
                    "grade_id": "cell-be966b4e10eb4ed8",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Overall, in this context there isn't a lot of difference between using the t-statistic versus the difference in means. (Remember that due to the small `n_trials` and `n_resamples` per trial, the numbers you get above can bounce around quite a bit. But if you re-run the tests a few times, or increase the `n_trials` and `n_resamples` parameters at the cost of longer execution time, you will see that the values are typically right in the same range...)\n",
                "\n",
                "## Question 2.6\n",
                "Let's compare the power of permutation test, bootstrap test (both using the t-statistic), and a parametric t-test. Paste your solution for `permutation_p` from the previous question below, and modify it to return just the p-value.\n",
                "\n",
                "A function to return a parametric t-test result is provided below.\n",
                "\n",
                "Use an effect size of 6, a sample size of 12, the default 250 trials, and `get_normal_sample`. Use 1000 permutations/resamples for the permutation/bootstrap tests respectively. Store the results in `boot_p`,  `perm_p`, and `parametric_p`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "c782da4052976782c75c1bbf536e55e8",
                    "grade": false,
                    "grade_id": "cell-2dfce5caba97461d",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def permutation_p(data_sets, statistic, n_permutations=10000):\n",
                "    # YOUR ANSWER HERE\n",
                "    return p_val\n",
                "\n",
                "from scipy import stats\n",
                "def t_test_parametric(data_sets):\n",
                "    # use the pre-canned two-sample t-test from the \"scipy.stats\" library. Scipy is a\n",
                "    # sister library to numpy, and it provides higher-level scientific computing tools,\n",
                "    # compared to the raw numerical computing primitives provided by numpy.\n",
                "    data1, data2 = data_sets\n",
                "    return stats.ttest_ind(data1, data2).pvalue\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "print('bootstrap power:', boot_p)\n",
                "print('permutation power:', perm_p)\n",
                "print('parametric power:', parametric_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "80fbdebf0c75fd51c35373d94a1a7fb0",
                    "grade": true,
                    "grade_id": "cell-9b31aab3ef644eba",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert abs(boot_p - 0.2) < 0.1\n",
                "assert abs(perm_p - 0.2) < 0.1\n",
                "assert abs(parametric_p - 0.2) < 0.1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "4e66cfceedae88602dea98af22f925ff",
                    "grade": false,
                    "grade_id": "cell-94e0a2596874fa97",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "As you can see, all of the tests are approximately similarly powered, even with a very small sample and a normal distribution (which should be the home turf for a parametric t-test). There are cases where a parametric t-test is theoretically better-powered than a permutation or bootstrap test, but they're hard to find. However, the real strength of the parametric test is that it is many, many times faster to execute than the simulation-based tests.\n",
                "\n",
                "## Question 2.7\n",
                "What about a case that's more suboptimal for the parametric test: one with a decidedly non-normal distribution?\n",
                "\n",
                "Re-run the above, but change the sample to `get_bimodal_sample` and increase the sample size to 20 (it turns out that with just 12 samples, the power is very low for all the tests...)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "d3036237142d18ae9f2096b30a947b3e",
                    "grade": false,
                    "grade_id": "cell-bb0afebc4335ae2a",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "# YOUR ANSWER HERE\n",
                "\n",
                "print('bootstrap power:', boot_p)\n",
                "print('permutation power:', perm_p)\n",
                "print('parametric power:', parametric_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "206235bd5b4440dc0ee5f90ef275cf77",
                    "grade": true,
                    "grade_id": "cell-b40b104aa278f4aa",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert abs(boot_p - 0.16) < 0.1\n",
                "assert abs(perm_p - 0.16) < 0.1\n",
                "assert abs(parametric_p - 0.16) < 0.1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "32040fe3031a5697d3a775c55e7a90c8",
                    "grade": false,
                    "grade_id": "cell-594da7d1b579c386",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "So in this case of non-normality, the parametric test does just fine: it's basically in the same range as the nonparametric tests. This is in large part due to the central limit theorem: differences between strangely-distributed data are generally distributed with something closer to a normal bell-curve shape.\n",
                "\n",
                "## Question 2.8\n",
                "Below is a much more pathological sample distribution: a case with large outliers that do not shift along with the effect size. You might get something like this with a noisy instrument that has some specific failure modes (crud that shows up as readings, or similar).\n",
                "\n",
                "The histogram that the code below produces shows the shape of this distribution.\n",
                "\n",
                "Run the above tests again, using an effect size of 16, a sample size of 100, the t-statistic as your statistic, and 1000 resamples/permutations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "90aa426ed00c82057a0163c734773fab",
                    "grade": false,
                    "grade_id": "cell-8ec24134a2f5bd69",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def get_outliers_bimodal_sample(sample_size, effect_size):\n",
                "    # draw a sample from a bimodal distribution\n",
                "    large_mode_size = int(sample_size * 2/3)\n",
                "    background_size = int(sample_size / 5)\n",
                "    small_mode_size = sample_size - large_mode_size - background_size\n",
                "    large_mode = numpy.abs(numpy.random.normal(size=large_mode_size, loc=effect_size, scale=7))\n",
                "    small_mode = numpy.random.normal(size=small_mode_size, loc=56, scale=3)\n",
                "    background_mode = numpy.random.uniform(0, 75, size=background_size)\n",
                "    return numpy.concatenate([large_mode, small_mode, background_mode])\n",
                "\n",
                "plt.hist(get_outliers_bimodal_sample(10000, 0), bins='auto')\n",
                "plt.hist(get_outliers_bimodal_sample(10000, 6), bins='auto', alpha=0.6)\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "print('bootstrap power:', boot_p)\n",
                "print('permutation power:', perm_p)\n",
                "print('parametric power:', parametric_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "079939141dfe83d7441434147c3b5bce",
                    "grade": true,
                    "grade_id": "cell-0a1f6019e785a44b",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert abs(boot_p - 0.78) < 0.1\n",
                "assert abs(perm_p - 0.78) < 0.1\n",
                "assert abs(parametric_p - 0.78) < 0.1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "8f856ab0bd6d180b1283402ddc67dd7f",
                    "grade": false,
                    "grade_id": "cell-cf246aa59ad3c22f",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Even in this textbook case of violated parametric t-test assumptions (a highly non symmetric sample distribution, with many outliers), it works just as well as the bootstrap and permutation tests.\n",
                "\n",
                "As we can see, the parametric t-test is quite robust to non-normally-distributed data -- especially for two-sample data where the difference between means is much closer to normally distributed. (Run the code below to see the distribution of the differences between the means of two independent samples of these data.)\n",
                "\n",
                "But even so  sometimes it's good to check funny-looking data with a nonparametric test like the bootstrap or the permutation too. Especially when using a one-sample test (a case we didn't examine here), where the central-limit theorem doesn't help!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "bcb902a1fccbe2e6a960ea1c0e3d3722",
                    "grade": false,
                    "grade_id": "cell-49a0492f6c031d5b",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "mean_diffs = [numpy.mean(get_outliers_bimodal_sample(100, 16)) - numpy.mean(get_outliers_bimodal_sample(100, 16)) for i in range(10000)]\n",
                "x = plt.hist(mean_diffs, bins='auto')"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}