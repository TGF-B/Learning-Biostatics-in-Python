{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "3ba681f30765a20effc42caaf7305738",
                    "grade": false,
                    "grade_id": "cell-36d0eea602658693",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "# Question 1: Permutation tests\n",
                "\n",
                "In class, we briefly mentioned permutation testing as an alternative to bootstrapping a two-sample test.\n",
                "\n",
                "Recall that with bootstrapping, you shift the data around to comply with the null hypothesis (often that the data sets have identical means). Then you repeatedly resample the transformed data and compute your statistic on the resampled data to generate a null distribution, which tells you how common it would be to see any given value of the statistic by chance if the data sets were actually not distinct.\n",
                "\n",
                "With permutation testing, we use the null hypothesis that the various data groups (i.e. \"control\" and \"experimental\", but this works for multi-group data as well) are actually all just drawn from a single big, indistinguishable group. If this were true, then it shouldn't matter which data points are assigned to which group -- because the groups are all identical anyway. Therefore, we can produce a null distribution by randomly shuffling (or \"permuting\") the groups to see how common it is to get any given difference between the groups by chance alone.\n",
                "\n",
                "Effectively, this is asking how often the observed configuration of the group labels (or one that yields similarly extreme values for some statistic) might come about simply due to chance alone. \n",
                "\n",
                "For this, we will use the same statistics and p-value functions as in the bootstrapping exercies. All that needs to be done to implement a permutation test is to write code that will reshuffle group labels -- or, equivalently, randomly assign each data point to one of the data groups.\n",
                "\n",
                "The simplest way to approach this problem is to assume that we have $i$ different groups, with sizes $n_i$, for a total of $k = \\sum_i n_i$ data points. If we concatenate all these data points together into a single list that is $k$ elements long, then the first $n_1$ data points will belong to group 1, the next $n_2$ points will belong to group 2, and so forth, up to the last $n_i$ points which belong to group $i$. \n",
                "\n",
                "For example, imagine  we have two groups of data: `control = [1, 4, 3, 5, 3]` (5 elements) and `experimental = [3, 5, 6, 5]` (4 elements). Put together, the concatenated data set is `all_data = [1, 4, 3, 5, 3, 3, 5, 6, 5]`, where `control = all_data[0:5]` and `experimental = all_data[5:9]`.\n",
                "\n",
                "If we shuffled the `all_data` list around (not by resampling with replacement! Just re-ordering the elements...), then our new \"control\" group would be `shuffled_data[0:5]` and our new \"experimental\" group would be `shuffled_data[5:9]`. Conveniently, the function `numpy.random.permutation` will return a permuted copy of its input array.\n",
                "\n",
                "Try the code out below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "c8944d4d34a73284ac27c995d3315cc7",
                    "grade": false,
                    "grade_id": "cell-a032b409d835148d",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "import numpy\n",
                "\n",
                "group1 = numpy.array([1, 1, 1, 1, 1, 1])\n",
                "group2 = numpy.array([2, 2, 2, 2])\n",
                "group3 = numpy.array([3, 3, 3, 3, 3, 3, 3, 3])\n",
                "groups = [group1, group2, group3]\n",
                "all_data = numpy.concatenate(groups)\n",
                "\n",
                "print('orig:', all_data)\n",
                "print()\n",
                "\n",
                "print('perm 1:', numpy.random.permutation(all_data))\n",
                "print('perm 2:', numpy.random.permutation(all_data))\n",
                "print('perm 3:', numpy.random.permutation(all_data))\n",
                "print()\n",
                "\n",
                "lengths = [len(group) for group in groups]\n",
                "print('lengths:', lengths)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "069f5c9b70d44a361f2a9ad01ed9f384",
                    "grade": false,
                    "grade_id": "cell-8b513929766cdf3d",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "## Question 1.0\n",
                "Write a function `split_data` that will split up an array or list into a set of sub-lists with specified lengths. We will use the function to split up a concatenated `all_data` array such as above back into groups of the requested sizes. \n",
                "\n",
                "Specifically, your function should be: `split_data(all_data, lengths)`, where `all_data` is a list or array, and `lengths` is a list of lengths of the required sub-lists. Note that `sum(lengths)` will always be equal to `len(all_data)` -- you could even enforce this via an assert.\n",
                "\n",
                "Here is an example of how the function should work:\n",
                "```python\n",
                "group1 = numpy.array([1])\n",
                "group2 = numpy.array([2, 3])\n",
                "group3 = numpy.array([4, 5, 6])\n",
                "groups = [group1, group2, group3]\n",
                "\n",
                "lengths = [len(group) for group in groups]\n",
                "# i.e. lengths = [1, 2, 3]\n",
                "all_data = numpy.concatenate(groups)\n",
                "\n",
                "split_groups = split_data(all_data, lengths)\n",
                "# split_groups should be [numpy.array([1]), numpy.array([2, 3]), numpy.array([4, 5, 6])\n",
                "```\n",
                "\n",
                "You can either do this with a for loop and a counter to record the index where the current sub-group starts in the `all_data` array, or (for a brainteaser / style points) you could look up the documentation for `numpy.cumsum` and `numpy.split` and figure out how to reduce the operation to one line."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "9fae03e80b8126869aa2424f11ec89e3",
                    "grade": false,
                    "grade_id": "cell-0db8d7f419872d13",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "import numpy\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "group1 = numpy.array([1])\n",
                "group2 = numpy.array([2, 3])\n",
                "group3 = numpy.array([4, 5, 6])\n",
                "groups = [group1, group2, group3]\n",
                "\n",
                "lengths = [len(group) for group in groups]\n",
                "all_data = numpy.concatenate(groups)\n",
                "\n",
                "split_groups = split_data(all_data, lengths)\n",
                "print(split_groups)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "a5bc461439e344cf0e4141c941673244",
                    "grade": true,
                    "grade_id": "cell-b9f77c4c291bad28",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "new_lengths = [2, 4, 6, 8, 10]\n",
                "all_data = numpy.arange(sum(new_lengths))\n",
                "split_groups = split_data(all_data, new_lengths)\n",
                "assert len(split_groups) == len(new_lengths)\n",
                "assert numpy.all(numpy.concatenate(split_groups) == all_data)\n",
                "for length, group in zip(new_lengths, split_groups):\n",
                "    assert len(group) == length"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "751b1f63fef1dcd34eaac3aecf7d96f2",
                    "grade": false,
                    "grade_id": "cell-ab0c03d902492832",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "## Question 1.1\n",
                "Using your `split_data` function and `numpy.random.permutation`, write a function `permutation_p(data_sets, statistic, n_permutations=10000)`, where:\n",
                " - `data_sets` is a list of two or more data sets\n",
                " - `statistic` is a function to calculate a statistic from two or more data sets\n",
                " - `n_permutations` is the number of permutations to examine\n",
                " \n",
                "The function should return `stat`, `permuted_stats`, `p`, where:\n",
                " - `stat` is the statistic calculated on the actual data sets\n",
                " - `permuted_stats` is the list of statistics computed on the permuted data\n",
                " - `p` is the two-tailed p-value.\n",
                "\n",
                "Several helper functions are provided below. Note that with a permutation test, the null hypothesis is always that a given statistic has zero difference between the groups."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4b0c52d2eb26a9946d468aa635ce73cd",
                    "grade": false,
                    "grade_id": "cell-7550fd9cf7af3a63",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def two_tail_p_value(actual_stat, resampled_stats, null_hyp_stat=0):\n",
                "    resampled_stats = numpy.asarray(resampled_stats)\n",
                "    actual_diff = abs(actual_stat - null_hyp_stat)\n",
                "    diff = numpy.abs(resampled_stats - null_hyp_stat)\n",
                "    count_extreme = numpy.count_nonzero(diff >= actual_diff)\n",
                "    return count_extreme / len(resampled_stats)\n",
                "\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "# load blood pressure data again...\n",
                "aa_bp = []\n",
                "gg_bp = []\n",
                "file = open('bloodpressure3.txt')\n",
                "header = file.readline()\n",
                "for line in file:\n",
                "    bp, genotype = line.strip('\\n').split('\\t')\n",
                "    bp = float(bp)\n",
                "    if genotype == 'AA':\n",
                "        aa_bp.append(bp)\n",
                "    elif genotype == 'GG':\n",
                "        gg_bp.append(bp)\n",
                "    else:\n",
                "        print('unknown genotype!', genotype)\n",
                "file.close()\n",
                "aa_bp = numpy.array(aa_bp)\n",
                "gg_bp = numpy.array(gg_bp)\n",
                "\n",
                "\n",
                "def mean_difference(data1, data2):\n",
                "    return numpy.mean(data1) - numpy.mean(data2)\n",
                "\n",
                "mean_diff, perm_diffs, perm_p = permutation_p([aa_bp, gg_bp], mean_difference)\n",
                "\n",
                "def resample(data_sets, statistic, n_resamples):\n",
                "    data_sets = [numpy.asarray(data_set) for data_set in data_sets]\n",
                "    stats = []\n",
                "    for i in range(n_resamples):\n",
                "        resampled_data_sets = [numpy.random.choice(data, size=len(data), replace=True) for data in data_sets]\n",
                "        stats.append(statistic(*resampled_data_sets))\n",
                "    return stats\n",
                "\n",
                "def bootstrap_means_different(data_sets, statistic, n_resamples=10000):\n",
                "    data_sets = [numpy.asarray(data_set) for data_set in data_sets]\n",
                "    actual_stat = statistic(*data_sets)\n",
                "    shifted = [data_set - numpy.mean(data_set) for data_set in data_sets]\n",
                "    resampled_stats = resample(shifted, statistic, n_resamples)\n",
                "    p_val = two_tail_p_value(actual_stat, resampled_stats)\n",
                "    return actual_stat, resampled_stats, p_val\n",
                "\n",
                "mean_diff, boot_diffs, boot_p = bootstrap_means_different([aa_bp, gg_bp], mean_difference)\n",
                "\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('ggplot')\n",
                "plt.rcParams['figure.figsize'] = [12, 4]\n",
                "\n",
                "plt.hist(perm_diffs, bins='auto', label='permutation null')\n",
                "plt.hist(boot_diffs, bins='auto', label='bootstrap null', alpha=0.6)\n",
                "plt.axvline(mean_diff, color='blue')\n",
                "plt.legend()\n",
                "print('permutation p:', perm_p)\n",
                "print('bootstrap p:', boot_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "e00c1f30c8647733142ec5af2f49e9fc",
                    "grade": true,
                    "grade_id": "cell-1eb7b4312bcf345d",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert len(perm_diffs) == 10000\n",
                "assert -0.1 < numpy.mean(perm_diffs) < 0.1\n",
                "assert 0.007 < perm_p < 0.014"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "56531001ea5c8ba5c1231fba7a1b6977",
                    "grade": false,
                    "grade_id": "cell-72cb78ce35b9073e",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Very interesting! The permutation test yields  consistently smaller p-values, at least in this case. Of course, this is only a good thing if these small p-values aren't false positives! We'll test that in the question about statistical power.\n",
                "\n",
                "The neat thing here is that the permutation code will work for almost any statistic. There is no requirement to transform the data, which has to be done slightly differently for each different statistic of interest. (We saw this in the last problem set: we had to modify the basic bootstrap p-value code depending on if we wanted to subtract the mean or the median of each data set, or divide by the standard deviation, or whatnot...)\n",
                "\n",
                "## Question 1.2\n",
                "Modify the above code to produce functions `median_difference` and `bootstrap_medians_different`. Then use your `permutation_p` and `bootstrap_medians_different` functions to perform the analysis as above, except on the difference in medians instead of means.\n",
                "\n",
                "Store the results of `permutation_p` in variables `median_diff, perm_diffs, perm_p`, and of `bootstrap_medians_different` in `median_diff, boot_diffs, boot_p`.\n",
                "\n",
                "The code below also plots the GG and AA histograms. Below we will compare them to understand how and why the p-values are different for the mean and median case!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "caef315ce9efe1c6dbc6ea11a3d9a850",
                    "grade": false,
                    "grade_id": "cell-ffa5496eed2fdea6",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "# YOUR ANSWER HERE\n",
                "\n",
                "plt.hist(perm_diffs, bins='auto', label='permutation null')\n",
                "plt.hist(boot_diffs, bins='auto', label='bootstrap null', alpha=0.6)\n",
                "plt.axvline(median_diff, color='blue')\n",
                "plt.legend()\n",
                "\n",
                "plt.figure()\n",
                "plt.hist(gg_bp, bins='auto', label='GG')\n",
                "plt.hist(aa_bp, bins='auto', alpha=0.6, label='AA')\n",
                "\n",
                "plt.axvline(numpy.mean(gg_bp), color='yellow', label='GG mean')\n",
                "plt.axvline(numpy.median(gg_bp), color='orange', label='GG median')\n",
                "plt.axvline(numpy.mean(aa_bp), color='cyan', label='AA mean')\n",
                "plt.axvline(numpy.median(aa_bp), color='blue', label='AA median')\n",
                "\n",
                "plt.legend()\n",
                "print('permutation p:', perm_p)\n",
                "print('bootstrap p:', boot_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4c61836df044fbfa490148ab5942031d",
                    "grade": true,
                    "grade_id": "cell-4ea78547f645484a",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.32 < perm_p < 0.36\n",
                "assert 0.42 < boot_p < 0.46"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "a10547671df7612514ea50fd670c3cc9",
                    "grade": false,
                    "grade_id": "cell-7a85348c9975625b",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "As you can see from the GG and AA histograms, there are a few outliers at the low tail of the AA dataset, giving it a slightly different shape than the GG histogram. (This is largely what makes the difference in the means is much larger than the difference in medians between these two data sets.)\n",
                "\n",
                "Does this have anything to do with why the shape of the null distribution is so different for the bootstrap and permutation tests? In particular, why is there such a large and asymmetric mass of probability around $\\textrm{median}(AA)-\\textrm{median}(GG)=-5$ for the bootstrapped null? We'll find out below.\n",
                "\n",
                "## Question 1.3\n",
                "Take 10,000 resamples of `gg_bp`, calculate the median of each, and store the result in a list `gg_medians`. Do the same for `aa_bp` and store it as `aa_medians`. \n",
                "\n",
                "Next, calculate the \"standard error of the median\" for each of these datasets, and store it as `sterr_gg` and `sterr_aa`. Remember that the standard error of a statistic is just the standard deviation of the distribution of where that statistic might appear in repeated samplings. Which of course we approximate by looking at where the statistic appears in repeated *resamplings* (such as were just calculated).\n",
                "\n",
                "The code will also plot histograms of these resamples. Is there anything interesting about the shape of the `aa_medians` histogram compared to `gg_medians`?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "bc65c633e5fed886ef79d23d911e3dcf",
                    "grade": false,
                    "grade_id": "cell-2a2b9f0d03566025",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "# YOUR ANSWER HERE\n",
                "\n",
                "plt.hist(gg_medians, bins='auto', label='GG resampled medians')\n",
                "plt.hist(aa_medians, bins='auto', alpha=0.6, label='AA resampled medians')\n",
                "plt.axvline(numpy.median(gg_bp), color='orange', label='GG median')\n",
                "plt.axvline(numpy.median(aa_bp), color='blue', label='AA median')\n",
                "plt.legend()\n",
                "print('standard error of the AA median', sterr_aa)\n",
                "print('standard error of the GG median', sterr_gg)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "0be6bf2f6d0c4ce506bd24a53e160698",
                    "grade": true,
                    "grade_id": "cell-6ab5464d419aacc9",
                    "locked": true,
                    "points": 4,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert numpy.median(gg_bp) == numpy.median(gg_medians)\n",
                "assert numpy.median(aa_bp) == numpy.median(aa_medians)\n",
                "assert len(gg_medians) == len(aa_medians) == 10000\n",
                "# the average resampled median is very close to the actual median for the GG data\n",
                "assert abs(numpy.median(gg_bp) - numpy.median(gg_medians)) < 0.1\n",
                "# the average resampled median is actually a little smaller than the actual median for the AA data\n",
                "assert 1.0 < numpy.median(aa_bp) - numpy.mean(aa_medians) < 1.2\n",
                "assert 2.8 < sterr_aa < 3.0\n",
                "assert 1.0 < sterr_gg < 1.2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "3737bd61870622a3a9d90e018d47f7fe",
                    "grade": false,
                    "grade_id": "cell-3c64cc5bf4fff23f",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "As you can see, the resampling process will occasionally draw enough of the low-valued outliers from the AA dataset to actually drag the median of an AA resample quite far from the true median of the AA data. (This  also causes a  much larger standard error for that dataset than the GG data.)\n",
                "\n",
                "In contrast, the permutation test always and only uses the original dataset. Thus, there is no chance of drawing an \"AA\" dataset with a large fraction of outliers like that. This leads to the more orderly, symmetric null distribution for the permutation, as seen in the histogram in question 1.2.\n",
                "\n",
                "So, which approach is right? Well, it depends! Is the slightly heavy tail in the AA dataset a sampling artifact that should be ignored? Or a true feature of the population with the AA genotype, which should be paid attention to? Statistics can't help us here!\n",
                "\n",
                "For the last two questions, we'll apply the permutation test to the other scenarios we did bootstrapping for in last homework: the ratio of the standard deviations, and an ANOVA analysis.\n",
                "\n",
                "## Question 1.4\n",
                "Let's test whether the AA and GG data have different standard deviations. Last time this required heavily modifying the bootstrapping code to ask about whether a ratio was larger or smaller than 1. Let's be a little more clever this time, and use a standard trick from statistics: the question of whether a ratio is larger or smaller than 1 is the same is the question of whether the logarithm of that ratio is positive or negative. (That is, as usual, one can transform a question concerning multiplicative scaling into one concerning additive scaling by using the logarithm.) \n",
                "\n",
                "Write a function `log_std_ratio(data1, data2)`, using `numpy.log` and `numpy.std` that will return the log of the ratio of the standard deviations of `data1` and `data2`. Use this to conduct a permutation test of the AA and GG standard deviations (make sure you order the data sets such that the ratio calculated is std(AA)/std(GG)...). Compare those results to the bootstrap test of the same (using the provided code, which is a simplification of the code you were asked to write for the last homework).\n",
                "\n",
                "Store the results of `permutation_p` in variables `log_ratio, perm_logs, perm_p`, and of `bootstrap_std_different` in `log_ratio, boot_logs, boot_p`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "ed667f4e1a36e3b3cc499fbf07a96f3b",
                    "grade": false,
                    "grade_id": "cell-baf7dde8477be28c",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "def log_std_ratio(data1, data2):\n",
                "    # YOUR ANSWER HERE\n",
                "\n",
                "def unit_stds(data1, data2):\n",
                "    data1 = numpy.asarray(data1)\n",
                "    data2 = numpy.asarray(data2)\n",
                "    mean1 = numpy.mean(data1)\n",
                "    mean2 = numpy.mean(data2)\n",
                "    unit1 = (data1 - mean1) / numpy.std(data1) + mean1\n",
                "    unit2 = (data2 - mean1) / numpy.std(data2) + mean2\n",
                "    return unit1, unit2\n",
                "\n",
                "def bootstrap_std_different(data1, data2, n_resamples=10000):\n",
                "    actual_log = log_std_ratio(data1, data2)\n",
                "    unit1, unit2 = unit_stds(data1, data2)\n",
                "    resampled_logs = resample([unit1, unit2], log_std_ratio, n_resamples)\n",
                "    p_val = two_tail_p_value(actual_log, resampled_logs)\n",
                "    return actual_log, resampled_logs, p_val\n",
                "\n",
                "\n",
                "# Now calculate p-values from a permutation and bootstrap test for the log std ratio.\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "\n",
                "plt.hist(perm_logs, bins='auto', label='permutation null')\n",
                "plt.hist(boot_logs, bins='auto', label='bootstrap null', alpha=0.6)\n",
                "plt.axvline(log_ratio, color='blue')\n",
                "plt.legend()\n",
                "print('std ratio:', numpy.std(aa_bp) / numpy.std(gg_bp))\n",
                "print('permutation p:', perm_p)\n",
                "print('bootstrap p:', boot_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "732e54514e1fba7277153fa402e61bba",
                    "grade": true,
                    "grade_id": "cell-8cac08d17a71934a",
                    "locked": true,
                    "points": 3,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert log_std_ratio(aa_bp, gg_bp) == log_ratio\n",
                "assert abs(numpy.exp(log_ratio) - numpy.std(aa_bp) / numpy.std(gg_bp)) < 0.0000001\n",
                "assert 0.028 < perm_p < 0.042\n",
                "assert 0.018 < boot_p < 0.032"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "7b04998e6703d13ef1e07d6c2193cd1a",
                    "grade": false,
                    "grade_id": "cell-c8ef25f3916163fe",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "Here, we see very similar results. (Now the bootstrap test is giving miniscually lower p-values than the permutation test, but things are very comparable.)\n",
                "\n",
                "## Question 1.5\n",
                "Finally, let's use the permutation test to do a four-sample ANOVA, again using the examples from the last homework.\n",
                "\n",
                "Store the results of `permutation_p` in variables `f_val, perm_fs, perm_p`, and of `bootstrap_anova` in `f_val, boot_fs, boot_p`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "eaba921d7ccf27c90c0301640aebb7ba",
                    "grade": false,
                    "grade_id": "cell-deb035f87d84b40f",
                    "locked": false,
                    "schema_version": 3,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "dose0 = numpy.array([\n",
                "        0.44353122,  0.01012966,  0.28873548,  0.41508564,  0.63920005,\n",
                "        0.38840745,  0.56994441,  0.39461312,  0.63531554,  0.2243476 ,\n",
                "        0.0146331 ,  0.34027282,  0.11461404,  0.3706856 ,  0.03022053,\n",
                "        0.26903881,  0.40752708,  0.57226634,  0.56036688,  0.45346642,\n",
                "        0.24393661,  0.11314218,  0.37424453,  0.02731236,  0.36170049,\n",
                "        0.28624436,  0.27673137,  0.02059751,  0.33590967,  0.34159704])\n",
                "\n",
                "dose1 = numpy.array([\n",
                "        0.40447368,  0.04894949,  0.43691414,  0.36156942,  0.55082948,\n",
                "        0.45234243,  0.08263355,  0.53735037,  0.19556559,  0.14684213,\n",
                "        0.12670269,  0.09968791,  0.09903378,  0.23750902,  0.30392521,\n",
                "        0.27720465,  0.64437095,  0.62903639,  0.62678483,  0.41837535,\n",
                "        0.02564389,  0.34572511,  0.48980329,  0.67060822,  0.40915237,\n",
                "        0.67581135,  0.63069843,  0.1503978 ,  0.12660994,  0.25990044])\n",
                "\n",
                "dose2 = numpy.array([\n",
                "        0.2023481 ,  0.17263792,  0.24061181,  0.23310464,  0.75607685,\n",
                "        0.67992754,  0.1684292 ,  0.72103724,  0.50828973,  0.19148194,\n",
                "        0.76176207,  0.31807931,  0.04664898,  0.22524644,  0.83347876,\n",
                "        0.58708986,  0.40128552,  0.33489851,  0.16898191,  0.0937998 ,\n",
                "        0.82937534,  0.69083288,  0.14723692,  0.51264047,  0.46377266,\n",
                "        0.55422885,  0.49082234,  0.80979359,  0.43772453,  0.35936454])\n",
                "\n",
                "dose3 = numpy.array([\n",
                "        0.56307295,  0.260053  ,  0.65498897,  0.4074656 ,  0.38936101,\n",
                "        0.10776682,  0.46990373,  0.39733009,  0.04181612,  0.15412012,\n",
                "        0.44600949,  0.56160325,  0.09421104,  0.29194078,  0.69317676,\n",
                "        0.22814752,  0.17328897,  0.40468055,  0.69985375,  0.39791401,\n",
                "        0.55387151,  0.68740186,  0.52072545,  0.65457568,  0.19796182,\n",
                "        0.58060724,  0.6489602 ,  0.2342047 ,  0.42521541,  0.62100328])\n",
                "\n",
                "plt.scatter([0]*30, dose0)\n",
                "plt.scatter([1]*30, dose1)\n",
                "plt.scatter([2]*30, dose2)\n",
                "plt.scatter([3]*30, dose3)\n",
                "\n",
                "def F_stat(*data_sets):\n",
                "    data_sets = [numpy.asarray(data) for data in data_sets]\n",
                "    grand_mean = numpy.mean(numpy.concatenate(data_sets))\n",
                "    # note: the sum of squares from a dataset to its mean is\n",
                "    # (per the definition of variance) the variance times the number of data points\n",
                "    ss_within = sum([numpy.var(data) * len(data) for data in data_sets])\n",
                "    ss_between = sum([len(data) * (numpy.mean(data) - grand_mean)**2 for data in data_sets])\n",
                "    return ss_between / ss_within\n",
                "\n",
                "def bootstrap_anova(data_sets, n_resamples=10000):\n",
                "    actual_stat = F_stat(*data_sets)\n",
                "    # set all means to zero instead of grand mean. Works the same\n",
                "    shifted = [data - numpy.mean(data) for data in data_sets]\n",
                "    resampled_stats = resample(shifted, F_stat, n_resamples)\n",
                "    p_val = two_tail_p_value(actual_stat, resampled_stats)\n",
                "    return actual_stat, resampled_stats, p_val\n",
                "\n",
                "# Now calculate p-values from a permutation and bootstrap test of the F statistic.\n",
                "# YOUR ANSWER HERE\n",
                "\n",
                "plt.figure()\n",
                "plt.hist(perm_fs, bins='auto', label='permutation null')\n",
                "plt.hist(boot_fs, bins='auto', label='bootstrap null', alpha=0.6)\n",
                "plt.axvline(f_val, color='blue')\n",
                "plt.legend()\n",
                "print('permutation p:', perm_p)\n",
                "print('bootstrap p:', boot_p)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "code",
                    "checksum": "4565bbda8256067cfdfd440793aecd83",
                    "grade": true,
                    "grade_id": "cell-0f0fff2546c8815a",
                    "locked": true,
                    "points": 2,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "outputs": [],
            "source": [
                "assert 0.1 < perm_p < 0.13\n",
                "assert 0.1 < boot_p < 0.13"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "deletable": false,
                "editable": false,
                "nbgrader": {
                    "cell_type": "markdown",
                    "checksum": "1df6043dab38371a4cf12b148ae11376",
                    "grade": false,
                    "grade_id": "cell-688980875b47a605",
                    "locked": true,
                    "schema_version": 3,
                    "solution": false
                }
            },
            "source": [
                "As you can see, the permutation test is very general for multi-sample tests, and works almost identically to bootstrap hypothesis testing. Nevertheless, it's important to understand how each works in order to understand the cases in which the tests differ a little bit. Both are useful and commonly-employed statistical methods, and so it's good to have both in your tool-belt."
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}